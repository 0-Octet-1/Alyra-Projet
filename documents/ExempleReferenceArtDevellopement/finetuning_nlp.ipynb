{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Fine-Tuning en NLP avec DistilBERT\n",
        "\n",
        "## üìö Introduction\n",
        "\n",
        "### üéØ Qu'est-ce que le Fine-Tuning en NLP ?\n",
        "\n",
        "Le fine-tuning en NLP consiste √† :\n",
        "\n",
        "1. **Partir d'un mod√®le pr√©-entra√Æn√©** (comme DistilBERT entra√Æn√© sur des millions de textes)\n",
        "2. **Ajouter une couche de classification** adapt√©e √† notre t√¢che sp√©cifique\n",
        "3. **R√©entra√Æner partiellement** le mod√®le sur nos donn√©es\n",
        "\n",
        "### üß† Pourquoi DistilBERT ?\n",
        "\n",
        "**DistilBERT** est une version \"distill√©e\" de BERT :\n",
        "\n",
        "‚úÖ **40% plus petit** que BERT original\n",
        "\n",
        "‚úÖ **60% plus rapide** √† l'ex√©cution\n",
        "\n",
        "‚úÖ **97% des performances** de BERT\n",
        "\n",
        "‚úÖ **Parfait pour l'apprentissage** et les applications en production\n",
        "\n",
        "### üé¨ Notre Projet : Analyse de Sentiment\n",
        "\n",
        "Dans ce notebook, nous allons :\n",
        "- Analyser le sentiment de critiques de films (positif/n√©gatif)\n",
        "- Utiliser le dataset IMDB (Internet Movie Database)\n",
        "- Appliquer le fine-tuning avec DistilBERT\n",
        "- Visualiser et comprendre chaque √©tape du processus\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configuration et Imports\n",
        "\n",
        "Commen√ßons par importer toutes les biblioth√®ques n√©cessaires et configurer notre environnement NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports principaux pour le NLP\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "import warnings\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Configuration pour un meilleur affichage\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(f\"üöÄ TensorFlow version: {tf.__version__}\")\n",
        "print(f\"üíª GPU disponible: {'‚úÖ Oui' if len(tf.config.list_physical_devices('GPU')) > 0 else '‚ùå Non (CPU utilis√©)'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üñ•Ô∏è Configuration GPU pour NLP\n",
        "\n",
        "Les mod√®les de NLP comme DistilBERT consomment beaucoup de m√©moire. Configurons le GPU prudemment :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration GPU sp√©ciale pour les mod√®les NLP volumineux\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Utiliser seulement le premier GPU\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        # Croissance m√©moire progressive (important pour les transformers)\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(f\"‚úÖ Configuration GPU r√©ussie: {gpus[0].name}\")\n",
        "        print(\"üß† M√©moire GPU configur√©e en croissance progressive\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur configuration GPU: {e}\")\n",
        "else:\n",
        "    print(\"üîß Utilisation du CPU - Les mod√®les NLP fonctionnent aussi (plus lentement)\")\n",
        "\n",
        "print(\"\\nüí° Info: Les mod√®les de transformers comme DistilBERT utilisent beaucoup de m√©moire\")\n",
        "print(\"   La croissance progressive √©vite les erreurs 'Out of Memory'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è Param√®tres d'Entra√Ænement pour NLP\n",
        "\n",
        "Les param√®tres pour le fine-tuning en NLP sont diff√©rents de ceux en vision :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Param√®tres sp√©cifiques au fine-tuning NLP\n",
        "BATCH_SIZE = 16            # Plus petit qu'en vision (mod√®les plus volumineux)\n",
        "MAX_LENGTH = 256           # Longueur maximale des s√©quences (tokens)\n",
        "EPOCHS = 3                 # Peu d'√©poques suffisent en fine-tuning NLP\n",
        "LEARNING_RATE = 2e-5       # Learning rate typique pour BERT/DistilBERT\n",
        "NUM_SAMPLES_TRAIN = 5000   # Limitation pour l'exemple p√©dagogique\n",
        "NUM_SAMPLES_VAL = 1000     # √âchantillon de validation\n",
        "\n",
        "print(\"üìã Configuration d'entra√Ænement NLP:\")\n",
        "print(f\"   ‚Ä¢ Taille de batch: {BATCH_SIZE} (plus petit pour les transformers)\")\n",
        "print(f\"   ‚Ä¢ Longueur max: {MAX_LENGTH} tokens\")\n",
        "print(f\"   ‚Ä¢ Nombre d'√©poques: {EPOCHS} (convergence rapide en fine-tuning)\")\n",
        "print(f\"   ‚Ä¢ Learning rate: {LEARNING_RATE} (optimal pour BERT)\")\n",
        "print(f\"   ‚Ä¢ √âchantillons train/val: {NUM_SAMPLES_TRAIN}/{NUM_SAMPLES_VAL}\")\n",
        "\n",
        "print(\"\\nüéì Diff√©rences avec la Computer Vision:\")\n",
        "print(\"   üîπ Batch size plus petit (mod√®les plus volumineux)\")\n",
        "print(\"   üîπ Moins d'√©poques (pas de surapprentissage)\")\n",
        "print(\"   üîπ Learning rate sp√©cifique aux transformers\")\n",
        "print(\"   üîπ Gestion de s√©quences de longueur variable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß† Comprendre le Fine-Tuning en NLP\n",
        "\n",
        "Avant de commencer, comprenons bien ce que nous allons faire :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé® Visualisation conceptuelle du fine-tuning NLP\n",
        "def visualiser_fine_tuning_nlp():\n",
        "    \"\"\"\n",
        "    Cr√©e un diagramme explicatif du fine-tuning en NLP\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
        "    \n",
        "    # D√©finir les composants et leurs positions\n",
        "    components = [\n",
        "        {\"name\": \"Texte d'entr√©e\\n'This movie is great!'\\n(Critique de film)\", \"pos\": (1, 9), \"color\": \"lightblue\", \"frozen\": False},\n",
        "        {\"name\": \"Tokenizer\\nDistilBERT\\n[CLS] this movie is great [SEP]\", \"pos\": (1, 7.5), \"color\": \"lightyellow\", \"frozen\": False},\n",
        "        {\"name\": \"DistilBERT\\n(Pr√©-entra√Æn√©)\\nüîí PARTIELLEMENT GEL√â\", \"pos\": (1, 6), \"color\": \"lightcoral\", \"frozen\": True},\n",
        "        {\"name\": \"Embeddings Contextuels\\n768 dimensions\\n(Repr√©sentation riche)\", \"pos\": (1, 4.5), \"color\": \"lightgreen\", \"frozen\": True},\n",
        "        {\"name\": \"Pooling Layer\\n[CLS] token\\n‚Üí Repr√©sentation globale\", \"pos\": (1, 3), \"color\": \"lightyellow\", \"frozen\": False},\n",
        "        {\"name\": \"Dropout(0.3)\\nüé≤ R√©gularisation\", \"pos\": (1, 1.5), \"color\": \"lightpink\", \"frozen\": False},\n",
        "        {\"name\": \"Dense(1)\\n+ Sigmoid\\nüéØ Classification Binaire\", \"pos\": (1, 0), \"color\": \"lightsteelblue\", \"frozen\": False}\n",
        "    ]\n",
        "    \n",
        "    # Dessiner les composants\n",
        "    for i, comp in enumerate(components):\n",
        "        x, y = comp[\"pos\"]\n",
        "        \n",
        "        # Style diff√©rent pour les couches gel√©es\n",
        "        if comp[\"frozen\"]:\n",
        "            bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=comp[\"color\"], \n",
        "                            edgecolor=\"red\", linewidth=2, linestyle=\"--\")\n",
        "        else:\n",
        "            bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=comp[\"color\"], \n",
        "                            edgecolor=\"black\", linewidth=1)\n",
        "        \n",
        "        ax.text(x, y, comp[\"name\"], ha=\"center\", va=\"center\", \n",
        "               fontsize=11, fontweight=\"bold\", bbox=bbox_props)\n",
        "        \n",
        "        # Fl√®ches entre composants\n",
        "        if i < len(components) - 1:\n",
        "            next_y = components[i+1][\"pos\"][1]\n",
        "            ax.annotate(\"\", xy=(x, next_y + 0.4), xytext=(x, y - 0.4),\n",
        "                       arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"darkblue\"))\n",
        "    \n",
        "    # Annotations explicatives\n",
        "    ax.text(3.5, 8, \"üí≠ √âtape 1: Tokenisation\\n‚Ä¢ Conversion texte ‚Üí tokens\\n‚Ä¢ Ajout tokens sp√©ciaux [CLS], [SEP]\\n‚Ä¢ Padding/Truncation √† 256 tokens\", \n",
        "           fontsize=10, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"wheat\", alpha=0.8))\n",
        "    \n",
        "    ax.text(3.5, 5.5, \"üß† √âtape 2: DistilBERT\\n‚Ä¢ Compr√©hension contextuelle\\n‚Ä¢ 66M param√®tres pr√©-entra√Æn√©s\\n‚Ä¢ Attention multi-t√™tes\", \n",
        "           fontsize=10, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcyan\", alpha=0.8))\n",
        "    \n",
        "    ax.text(3.5, 2, \"üéØ √âtape 3: Classification\\n‚Ä¢ Token [CLS] ‚Üí repr√©sentation globale\\n‚Ä¢ Nouvelle t√¢che: sentiment\\n‚Ä¢ Apprentissage adaptatif\", \n",
        "           fontsize=10, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lavender\", alpha=0.8))\n",
        "    \n",
        "    # Configuration des axes\n",
        "    ax.set_xlim(-0.5, 6)\n",
        "    ax.set_ylim(-1, 10)\n",
        "    ax.set_title(\"Architecture du Fine-Tuning NLP avec DistilBERT\", fontsize=16, fontweight=\"bold\", pad=20)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # L√©gende\n",
        "    legend_elements = [\n",
        "        plt.Rectangle((0, 0), 1, 1, facecolor='lightcoral', edgecolor='red', linestyle='--', label='Couches pr√©-entra√Æn√©es'),\n",
        "        plt.Rectangle((0, 0), 1, 1, facecolor='lightsteelblue', edgecolor='black', label='Nouvelles couches')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='upper right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"üé® Visualisation de l'architecture NLP:\")\n",
        "visualiser_fine_tuning_nlp()\n",
        "\n",
        "print(\"\\nüí° Points cl√©s du fine-tuning NLP:\")\n",
        "print(\"   üß† DistilBERT comprend d√©j√† le langage (pr√©-entra√Æn√©)\")\n",
        "print(\"   üéØ On ajoute juste une t√¢che sp√©cifique (classification sentiment)\")\n",
        "print(\"   ‚ö° Convergence tr√®s rapide (2-3 √©poques)\")\n",
        "print(\"   üé≤ Attention √† la r√©gularisation (mod√®les puissants)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé¨ Chargement des Donn√©es - Dataset IMDB\n",
        "\n",
        "Nous allons utiliser le fameux dataset **IMDB** (Internet Movie Database) qui contient des critiques de films avec leur sentiment associ√©.\n",
        "\n",
        "### üìä Qu'est-ce que le dataset IMDB ?\n",
        "\n",
        "- **25,000 critiques** d'entra√Ænement + 25,000 de test\n",
        "- **2 classes** : Positif (üëç) et N√©gatif (üëé)\n",
        "- **Textes de longueur variable** (quelques mots √† plusieurs paragraphes)\n",
        "- **Langue anglaise** avec vocabulaire riche et vari√©\n",
        "- **R√©f√©rence en NLP** pour l'analyse de sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé¨ Chargement du dataset IMDB\n",
        "print(\"üì• T√©l√©chargement du dataset IMDB...\")\n",
        "print(\"(Cela peut prendre quelques minutes la premi√®re fois)\\n\")\n",
        "\n",
        "# Chargement avec TensorFlow Datasets\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split=['train[:80%]', 'train[80%:]'],  # 80% train, 20% validation\n",
        "    with_info=True,\n",
        "    as_supervised=True  # Retourne (texte, label)\n",
        ")\n",
        "\n",
        "# üìä Informations sur le dataset\n",
        "print(f\"‚úÖ Dataset IMDB charg√© avec succ√®s !\")\n",
        "print(f\"üìà Classes: {info.features['label'].names}\")\n",
        "print(f\"üìä Total d'exemples: {info.splits['train'].num_examples}\")\n",
        "print(f\"üî¢ Distribution: √âquilibr√©e (50% positif, 50% n√©gatif)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Conversion et Limitation des Donn√©es\n",
        "\n",
        "Pour cet exemple p√©dagogique, nous allons limiter le nombre d'exemples pour un entra√Ænement plus rapide :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ Conversion des donn√©es en listes Python\n",
        "print(\"üîß Pr√©paration des donn√©es...\")\n",
        "\n",
        "# Extraction des textes et labels d'entra√Ænement\n",
        "print(f\"üì• Extraction de {NUM_SAMPLES_TRAIN} exemples d'entra√Ænement...\")\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "\n",
        "for text, label in train_data.take(NUM_SAMPLES_TRAIN):\n",
        "    train_texts.append(text.numpy().decode('utf-8'))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "# Extraction des textes et labels de validation\n",
        "print(f\"üì• Extraction de {NUM_SAMPLES_VAL} exemples de validation...\")\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "\n",
        "for text, label in test_data.take(NUM_SAMPLES_VAL):\n",
        "    test_texts.append(text.numpy().decode('utf-8'))\n",
        "    test_labels.append(label.numpy())\n",
        "\n",
        "# Conversion en arrays NumPy\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"\\n‚úÖ Donn√©es pr√©par√©es:\")\n",
        "print(f\"   üìö Entra√Ænement: {len(train_texts)} exemples\")\n",
        "print(f\"   üéØ Validation: {len(test_texts)} exemples\")\n",
        "print(f\"   üìä Ratio train/val: {len(train_texts)/len(test_texts):.1f}:1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù Exploration des Donn√©es\n",
        "\n",
        "Regardons de plus pr√®s le contenu de notre dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Analyse des donn√©es\n",
        "print(\"üîç Analyse du contenu des donn√©es:\\n\")\n",
        "\n",
        "# Distribution des classes\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "print(\"üìä Distribution des classes (entra√Ænement):\")\n",
        "for class_id, count in zip(unique, counts):\n",
        "    class_name = \"Positif üëç\" if class_id == 1 else \"N√©gatif üëé\"\n",
        "    percentage = count / len(train_labels) * 100\n",
        "    print(f\"   {class_name}: {count} exemples ({percentage:.1f}%)\")\n",
        "\n",
        "# Statistiques sur la longueur des textes\n",
        "text_lengths = [len(text.split()) for text in train_texts]\n",
        "print(f\"\\nüìè Statistiques de longueur (en mots):\")\n",
        "print(f\"   üìà Moyenne: {np.mean(text_lengths):.1f} mots\")\n",
        "print(f\"   üìä M√©diane: {np.median(text_lengths):.0f} mots\")\n",
        "print(f\"   üìâ Min: {min(text_lengths)} mots\")\n",
        "print(f\"   üìà Max: {max(text_lengths)} mots\")\n",
        "print(f\"   üìä 75e percentile: {np.percentile(text_lengths, 75):.0f} mots\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìö Exemples de Critiques\n",
        "\n",
        "Examinons quelques exemples concrets pour mieux comprendre notre dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìù Affichage d'exemples repr√©sentatifs\n",
        "def afficher_exemples(texts, labels, num_exemples=4):\n",
        "    \"\"\"\n",
        "    Affiche des exemples de critiques avec leur sentiment\n",
        "    \"\"\"\n",
        "    print(\"üìù Exemples de critiques de films:\\n\")\n",
        "    \n",
        "    for i in range(min(num_exemples, len(texts))):\n",
        "        # Informations sur l'exemple\n",
        "        sentiment = \"Positif üëç\" if labels[i] == 1 else \"N√©gatif üëé\"\n",
        "        text = texts[i]\n",
        "        word_count = len(text.split())\n",
        "        \n",
        "        print(f\"‚îÅ‚îÅ‚îÅ Exemple {i+1} ‚îÅ‚îÅ‚îÅ\")\n",
        "        print(f\"üè∑Ô∏è Sentiment: {sentiment}\")\n",
        "        print(f\"üìè Longueur: {word_count} mots\")\n",
        "        print(f\"üìÑ Texte: {text[:300]}{'...' if len(text) > 300 else ''}\")\n",
        "        print()\n",
        "\n",
        "# Afficher des exemples d'entra√Ænement\n",
        "afficher_exemples(train_texts, train_labels, 3)\n",
        "\n",
        "print(\"üí° Observations:\")\n",
        "print(\"   ‚Ä¢ Les critiques varient √©norm√©ment en longueur\")\n",
        "print(\"   ‚Ä¢ Le vocabulaire est riche et expressif\")\n",
        "print(\"   ‚Ä¢ Pr√©sence d'expressions idiomatiques et de sarcasme\")\n",
        "print(\"   ‚Ä¢ Certaines critiques sont tr√®s courtes, d'autres tr√®s d√©taill√©es\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Visualisation de la Distribution des Longueurs\n",
        "\n",
        "Cr√©ons des graphiques pour mieux comprendre nos donn√©es :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Visualisation des statistiques des donn√©es\n",
        "def visualiser_donnees_imdb(texts, labels):\n",
        "    \"\"\"\n",
        "    Cr√©e des visualisations pour comprendre le dataset IMDB\n",
        "    \"\"\"\n",
        "    # Calculs pr√©liminaires\n",
        "    text_lengths = [len(text.split()) for text in texts]\n",
        "    char_lengths = [len(text) for text in texts]\n",
        "    \n",
        "    # Cr√©er la figure avec plusieurs sous-graphiques\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('üìä Analyse du Dataset IMDB - Critiques de Films', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Distribution des classes\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    class_names = ['N√©gatif üëé', 'Positif üëç']\n",
        "    colors = ['salmon', 'lightgreen']\n",
        "    \n",
        "    axes[0,0].pie(counts, labels=class_names, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "    axes[0,0].set_title('Distribution des Sentiments', fontweight='bold')\n",
        "    \n",
        "    # 2. Distribution des longueurs (en mots)\n",
        "    axes[0,1].hist(text_lengths, bins=50, color='skyblue', alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].axvline(np.mean(text_lengths), color='red', linestyle='--', label=f'Moyenne: {np.mean(text_lengths):.0f}')\n",
        "    axes[0,1].axvline(MAX_LENGTH, color='orange', linestyle='--', label=f'Limite: {MAX_LENGTH}')\n",
        "    axes[0,1].set_title('Distribution des Longueurs (mots)', fontweight='bold')\n",
        "    axes[0,1].set_xlabel('Nombre de mots')\n",
        "    axes[0,1].set_ylabel('Fr√©quence')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Box plot par sentiment\n",
        "    pos_lengths = [text_lengths[i] for i, label in enumerate(labels) if label == 1]\n",
        "    neg_lengths = [text_lengths[i] for i, label in enumerate(labels) if label == 0]\n",
        "    \n",
        "    axes[1,0].boxplot([neg_lengths, pos_lengths], labels=['N√©gatif üëé', 'Positif üëç'])\n",
        "    axes[1,0].set_title('Longueur des Textes par Sentiment', fontweight='bold')\n",
        "    axes[1,0].set_ylabel('Nombre de mots')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Distribution cumulative\n",
        "    sorted_lengths = np.sort(text_lengths)\n",
        "    y = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths)\n",
        "    axes[1,1].plot(sorted_lengths, y, 'b-', linewidth=2)\n",
        "    axes[1,1].axvline(MAX_LENGTH, color='orange', linestyle='--', label=f'Limite: {MAX_LENGTH}')\n",
        "    axes[1,1].set_title('Distribution Cumulative des Longueurs', fontweight='bold')\n",
        "    axes[1,1].set_xlabel('Nombre de mots')\n",
        "    axes[1,1].set_ylabel('Proportion cumulative')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    axes[1,1].legend()\n",
        "    \n",
        "    # Pourcentage de textes sous la limite\n",
        "    under_limit = sum(1 for length in text_lengths if length <= MAX_LENGTH) / len(text_lengths) * 100\n",
        "    axes[1,1].text(0.6, 0.2, f'{under_limit:.1f}% des textes\\nsont ‚â§ {MAX_LENGTH} mots', \n",
        "                  transform=axes[1,1].transAxes, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Statistiques importantes\n",
        "    print(f\"\\nüìä Statistiques importantes:\")\n",
        "    print(f\"   üìè {under_limit:.1f}% des textes sont ‚â§ {MAX_LENGTH} mots\")\n",
        "    print(f\"   ‚úÇÔ∏è {100-under_limit:.1f}% seront tronqu√©s lors de la tokenisation\")\n",
        "    print(f\"   üìà Longueur moyenne: {np.mean(text_lengths):.1f} mots\")\n",
        "    print(f\"   üìä √âcart-type: {np.std(text_lengths):.1f} mots\")\n",
        "\n",
        "# Cr√©er les visualisations\n",
        "print(\"üìä Cr√©ation des visualisations du dataset IMDB:\")\n",
        "visualiser_donnees_imdb(train_texts, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üî§ Analyse du Vocabulaire\n",
        "\n",
        "Regardons la richesse du vocabulaire dans notre dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî§ Analyse du vocabulaire\n",
        "def analyser_vocabulaire(texts, top_n=20):\n",
        "    \"\"\"\n",
        "    Analyse le vocabulaire du dataset\n",
        "    \"\"\"\n",
        "    print(\"üî§ Analyse du vocabulaire:\")\n",
        "    \n",
        "    # Compter tous les mots\n",
        "    from collections import Counter\n",
        "    import re\n",
        "    \n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        # Nettoyage simple et tokenisation\n",
        "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "        all_words.extend(words)\n",
        "    \n",
        "    word_counts = Counter(all_words)\n",
        "    unique_words = len(word_counts)\n",
        "    total_words = len(all_words)\n",
        "    \n",
        "    print(f\"\\nüìä Statistiques du vocabulaire:\")\n",
        "    print(f\"   üî¢ Mots uniques: {unique_words:,}\")\n",
        "    print(f\"   üìö Total de mots: {total_words:,}\")\n",
        "    print(f\"   üìä Richesse lexicale: {unique_words/total_words:.3f}\")\n",
        "    \n",
        "    # Top mots les plus fr√©quents\n",
        "    print(f\"\\nüèÜ Top {top_n} mots les plus fr√©quents:\")\n",
        "    for i, (word, count) in enumerate(word_counts.most_common(top_n), 1):\n",
        "        percentage = count / total_words * 100\n",
        "        print(f\"   {i:2d}. '{word}': {count:,} occurrences ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Visualisation des fr√©quences\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Graphique 1: Top mots\n",
        "    plt.subplot(1, 2, 1)\n",
        "    top_words = word_counts.most_common(15)\n",
        "    words, counts = zip(*top_words)\n",
        "    plt.barh(range(len(words)), counts, color='skyblue')\n",
        "    plt.yticks(range(len(words)), words)\n",
        "    plt.xlabel('Fr√©quence')\n",
        "    plt.title(f'Top {len(words)} Mots les Plus Fr√©quents', fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # Graphique 2: Distribution des fr√©quences\n",
        "    plt.subplot(1, 2, 2)\n",
        "    frequencies = list(word_counts.values())\n",
        "    plt.hist(frequencies, bins=50, color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel('Fr√©quence des mots')\n",
        "    plt.ylabel('Nombre de mots')\n",
        "    plt.title('Distribution des Fr√©quences', fontweight='bold')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return word_counts\n",
        "\n",
        "# Analyser le vocabulaire\n",
        "vocab_stats = analyser_vocabulaire(train_texts)\n",
        "\n",
        "print(\"\\nüí° Observations sur le vocabulaire:\")\n",
        "print(\"   ‚Ä¢ Pr√©sence de mots tr√®s fr√©quents (articles, pr√©positions)\")\n",
        "print(\"   ‚Ä¢ Vocabulaire sp√©cialis√© du cin√©ma ('movie', 'film', 'plot')\")\n",
        "print(\"   ‚Ä¢ Mots expressifs pour les sentiments ('great', 'terrible', 'amazing')\")\n",
        "print(\"   ‚Ä¢ Distribution typique: peu de mots tr√®s fr√©quents, beaucoup de mots rares\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî§ Tokenisation avec DistilBERT\n",
        "\n",
        "La **tokenisation** est l'√©tape qui transforme le texte brut en tokens num√©riques que le mod√®le peut comprendre.\n",
        "\n",
        "### üß† Qu'est-ce que la Tokenisation ?\n",
        "\n",
        "La tokenisation :\n",
        "- **Divise le texte** en unit√©s plus petites (tokens)\n",
        "- **Convertit les mots** en indices num√©riques\n",
        "- **Ajoute des tokens sp√©ciaux** ([CLS], [SEP], [PAD])\n",
        "- **G√®re la longueur** (padding/truncation)\n",
        "\n",
        "### üéØ Sp√©cificit√©s du Tokenizer DistilBERT\n",
        "\n",
        "- **WordPiece tokenization** : Divise les mots rares en sous-mots\n",
        "- **Vocabulaire de ~30k tokens** : Couvre la plupart des mots anglais\n",
        "- **Tokens sp√©ciaux** :\n",
        "  - `[CLS]` : D√©but de s√©quence (classification)\n",
        "  - `[SEP]` : Fin de s√©quence\n",
        "  - `[PAD]` : Padding pour √©galiser les longueurs\n",
        "  - `[UNK]` : Mots inconnus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî§ Chargement du tokenizer DistilBERT\n",
        "print(\"üî§ Chargement du tokenizer DistilBERT...\")\n",
        "\n",
        "# Chargement depuis Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "print(\"‚úÖ Tokenizer DistilBERT charg√© avec succ√®s !\")\n",
        "print(f\"üìä Taille du vocabulaire: {tokenizer.vocab_size:,} tokens\")\n",
        "print(f\"üî§ Mod√®le: distilbert-base-uncased (anglais, minuscules)\")\n",
        "\n",
        "# Informations sur les tokens sp√©ciaux\n",
        "print(f\"\\nüéØ Tokens sp√©ciaux:\")\n",
        "print(f\"   üèÅ CLS (d√©but): '{tokenizer.cls_token}' (ID: {tokenizer.cls_token_id})\")\n",
        "print(f\"   üîö SEP (fin): '{tokenizer.sep_token}' (ID: {tokenizer.sep_token_id})\")\n",
        "print(f\"   üìù PAD (padding): '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
        "print(f\"   ‚ùì UNK (inconnu): '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß™ D√©monstration de la Tokenisation\n",
        "\n",
        "Voyons comment le tokenizer transforme du texte en nombres :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ D√©monstration de la tokenisation\n",
        "def demonstrer_tokenisation(tokenizer, exemples_textes):\n",
        "    \"\"\"\n",
        "    D√©montre le processus de tokenisation √©tape par √©tape\n",
        "    \"\"\"\n",
        "    print(\"üß™ === D√âMONSTRATION DE LA TOKENISATION ===\")\n",
        "    \n",
        "    for i, texte in enumerate(exemples_textes):\n",
        "        print(f\"\\nüìù Exemple {i+1}: '{texte}'\")\n",
        "        print(\"‚îÄ\" * 60)\n",
        "        \n",
        "        # √âtape 1: Tokenisation basique\n",
        "        tokens = tokenizer.tokenize(texte)\n",
        "        print(f\"üî§ Tokens: {tokens}\")\n",
        "        print(f\"üìä Nombre de tokens: {len(tokens)}\")\n",
        "        \n",
        "        # √âtape 2: Conversion en IDs\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        print(f\"üî¢ IDs des tokens: {token_ids}\")\n",
        "        \n",
        "        # √âtape 3: Encoding complet (avec tokens sp√©ciaux)\n",
        "        encoded = tokenizer.encode(texte, add_special_tokens=True)\n",
        "        print(f\"üéØ Encodage complet: {encoded}\")\n",
        "        \n",
        "        # √âtape 4: D√©codage pour v√©rifier\n",
        "        decoded = tokenizer.decode(encoded)\n",
        "        print(f\"üîÑ D√©codage: '{decoded}'\")\n",
        "        \n",
        "        # Explication des ajouts\n",
        "        if len(encoded) > len(token_ids):\n",
        "            print(f\"‚ûï Tokens ajout√©s: [CLS] au d√©but, [SEP] √† la fin\")\n",
        "\n",
        "# Exemples vari√©s pour la d√©monstration\n",
        "exemples = [\n",
        "    \"This movie is great!\",\n",
        "    \"Absolutely terrible film\",\n",
        "    \"The cinematography was breathtaking\"\n",
        "]\n",
        "\n",
        "demonstrer_tokenisation(tokenizer, exemples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è Fonction de Tokenisation pour nos Donn√©es\n",
        "\n",
        "Cr√©ons la fonction qui va tokeniser tous nos textes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è Fonction de tokenisation pour l'entra√Ænement\n",
        "def tokenize_texts(texts, tokenizer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Tokenise une liste de textes avec padding et truncation\n",
        "    \n",
        "    Args:\n",
        "        texts: Liste de textes √† tokeniser\n",
        "        tokenizer: Tokenizer DistilBERT\n",
        "        max_length: Longueur maximale des s√©quences\n",
        "    \n",
        "    Returns:\n",
        "        Dict contenant input_ids et attention_mask\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,              # Ajouter du padding pour √©galiser les longueurs\n",
        "        truncation=True,           # Tronquer les textes trop longs\n",
        "        max_length=max_length,     # Longueur maximale\n",
        "        return_tensors='tf'        # Retourner des tenseurs TensorFlow\n",
        "    )\n",
        "\n",
        "print(\"‚öôÔ∏è Fonction de tokenisation d√©finie\")\n",
        "print(f\"\\nüéØ Param√®tres de tokenisation:\")\n",
        "print(f\"   üìè Longueur maximale: {MAX_LENGTH} tokens\")\n",
        "print(f\"   üìù Padding: Activ√© (√©galise toutes les s√©quences)\")\n",
        "print(f\"   ‚úÇÔ∏è Truncation: Activ√©e (coupe les textes trop longs)\")\n",
        "print(f\"   üîß Format de sortie: Tenseurs TensorFlow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîÑ Tokenisation de nos Donn√©es\n",
        "\n",
        "Appliquons maintenant la tokenisation √† tous nos textes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ Tokenisation des donn√©es d'entra√Ænement et de validation\n",
        "print(\"üîÑ Tokenisation des donn√©es...\\n\")\n",
        "\n",
        "# Tokenisation des donn√©es d'entra√Ænement\n",
        "print(f\"üìö Tokenisation de {len(train_texts)} textes d'entra√Ænement...\")\n",
        "train_encodings = tokenize_texts(train_texts, tokenizer)\n",
        "print(\"‚úÖ Entra√Ænement tokenis√©\")\n",
        "\n",
        "# Tokenisation des donn√©es de validation\n",
        "print(f\"üéØ Tokenisation de {len(test_texts)} textes de validation...\")\n",
        "test_encodings = tokenize_texts(test_texts, tokenizer)\n",
        "print(\"‚úÖ Validation tokenis√©e\")\n",
        "\n",
        "print(f\"\\nüìä R√©sultats de la tokenisation:\")\n",
        "print(f\"   üìà Train - input_ids shape: {train_encodings['input_ids'].shape}\")\n",
        "print(f\"   üìà Train - attention_mask shape: {train_encodings['attention_mask'].shape}\")\n",
        "print(f\"   üìä Test - input_ids shape: {test_encodings['input_ids'].shape}\")\n",
        "print(f\"   üìä Test - attention_mask shape: {test_encodings['attention_mask'].shape}\")\n",
        "\n",
        "print(f\"\\nüí° Explication des sorties:\")\n",
        "print(f\"   üî¢ input_ids: Les tokens convertis en nombres\")\n",
        "print(f\"   üëÅÔ∏è attention_mask: Masque pour ignorer le padding (1=vrai token, 0=padding)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Analyse de la Tokenisation\n",
        "\n",
        "Examinons le r√©sultat de la tokenisation sur quelques exemples :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Analyse d√©taill√©e de la tokenisation\n",
        "def analyser_tokenisation(texts, encodings, tokenizer, num_exemples=3):\n",
        "    \"\"\"\n",
        "    Analyse d√©taill√©e du r√©sultat de la tokenisation\n",
        "    \"\"\"\n",
        "    print(\"üîç === ANALYSE D√âTAILL√âE DE LA TOKENISATION ===\")\n",
        "    \n",
        "    for i in range(min(num_exemples, len(texts))):\n",
        "        print(f\"\\nüìù Exemple {i+1}:\")\n",
        "        print(\"‚ïê\" * 70)\n",
        "        \n",
        "        # Texte original\n",
        "        original_text = texts[i]\n",
        "        print(f\"üìÑ Texte original: '{original_text[:100]}{'...' if len(original_text) > 100 else ''}'\")\n",
        "        print(f\"üìè Longueur originale: {len(original_text)} caract√®res, {len(original_text.split())} mots\")\n",
        "        \n",
        "        # Donn√©es tokenis√©es\n",
        "        input_ids = encodings['input_ids'][i].numpy()\n",
        "        attention_mask = encodings['attention_mask'][i].numpy()\n",
        "        \n",
        "        print(f\"\\nüî¢ Input IDs: {input_ids[:20]}{'...' if len(input_ids) > 20 else ''}\")\n",
        "        print(f\"üëÅÔ∏è Attention mask: {attention_mask[:20]}{'...' if len(attention_mask) > 20 else ''}\")\n",
        "        \n",
        "        # D√©codage\n",
        "        decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
        "        print(f\"üîÑ Texte d√©cod√©: '{decoded_text}'\")\n",
        "        \n",
        "        # Statistiques\n",
        "        non_padding_tokens = np.sum(attention_mask)\n",
        "        padding_tokens = len(attention_mask) - non_padding_tokens\n",
        "        \n",
        "        print(f\"\\nüìä Statistiques:\")\n",
        "        print(f\"   üéØ Tokens r√©els: {non_padding_tokens}\")\n",
        "        print(f\"   üìù Tokens de padding: {padding_tokens}\")\n",
        "        print(f\"   üìè Longueur totale: {len(input_ids)}\")\n",
        "        print(f\"   üìä Ratio padding: {padding_tokens/len(input_ids)*100:.1f}%\")\n",
        "        \n",
        "        # Analyse des tokens sp√©ciaux\n",
        "        special_tokens_found = []\n",
        "        if tokenizer.cls_token_id in input_ids:\n",
        "            special_tokens_found.append(\"[CLS]\")\n",
        "        if tokenizer.sep_token_id in input_ids:\n",
        "            special_tokens_found.append(\"[SEP]\")\n",
        "        if tokenizer.pad_token_id in input_ids:\n",
        "            special_tokens_found.append(\"[PAD]\")\n",
        "        \n",
        "        print(f\"   üéØ Tokens sp√©ciaux: {', '.join(special_tokens_found)}\")\n",
        "\n",
        "# Analyser quelques exemples\n",
        "analyser_tokenisation(train_texts, train_encodings, tokenizer, 2)\n",
        "\n",
        "print(\"\\nüí° Points cl√©s de la tokenisation:\")\n",
        "print(\"   üî§ Chaque texte commence par [CLS] et finit par [SEP]\")\n",
        "print(\"   üìù Les textes courts sont compl√©t√©s avec [PAD]\")\n",
        "print(\"   ‚úÇÔ∏è Les textes longs sont tronqu√©s √† MAX_LENGTH\")\n",
        "print(\"   üëÅÔ∏è L'attention mask aide le mod√®le √† ignorer le padding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Cr√©ation des Datasets TensorFlow\n",
        "\n",
        "Maintenant que nos textes sont tokenis√©s, cr√©ons les datasets TensorFlow optimis√©s pour l'entra√Ænement.\n",
        "\n",
        "### üéØ Pourquoi tf.data.Dataset ?\n",
        "\n",
        "- **Performance optimis√©e** : Lecture et pr√©processing efficaces\n",
        "- **Batching automatique** : Groupement des exemples\n",
        "- **Prefetching** : Chargement en parall√®le de l'entra√Ænement\n",
        "- **M√©lange des donn√©es** : √âvite la m√©morisation de l'ordre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Cr√©ation des datasets TensorFlow\n",
        "print(\"üì¶ Cr√©ation des datasets TensorFlow optimis√©s...\\n\")\n",
        "\n",
        "# Dataset d'entra√Ænement\n",
        "print(\"üèóÔ∏è Construction du dataset d'entra√Ænement...\")\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_encodings['input_ids'],\n",
        "        'attention_mask': train_encodings['attention_mask']\n",
        "    },\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "# Optimisations pour l'entra√Ænement\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000)  # M√©langer les donn√©es\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)          # Cr√©er des batches\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE) # Pr√©chargement parall√®le\n",
        "\n",
        "print(\"‚úÖ Dataset d'entra√Ænement cr√©√©\")\n",
        "\n",
        "# Dataset de validation\n",
        "print(\"üéØ Construction du dataset de validation...\")\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': test_encodings['input_ids'],\n",
        "        'attention_mask': test_encodings['attention_mask']\n",
        "    },\n",
        "    test_labels\n",
        "))\n",
        "\n",
        "# Optimisations pour la validation (pas de shuffle)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"‚úÖ Dataset de validation cr√©√©\")\n",
        "\n",
        "print(f\"\\nüìä Configuration des datasets:\")\n",
        "print(f\"   üìà Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   üîÄ Shuffle train: Oui (buffer=1000)\")\n",
        "print(f\"   üéØ Shuffle test: Non (ordre pr√©serv√©)\")\n",
        "print(f\"   ‚ö° Prefetch: AUTOTUNE (parall√©lisation)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Exploration des Datasets\n",
        "\n",
        "V√©rifions que nos datasets sont correctement configur√©s :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç Exploration des datasets cr√©√©s\n",
        "def explorer_dataset(dataset, nom_dataset, num_batches=1):\n",
        "    \"\"\"\n",
        "    Explore la structure d'un dataset TensorFlow\n",
        "    \"\"\"\n",
        "    print(f\"üîç === EXPLORATION DU DATASET {nom_dataset.upper()} ===\")\n",
        "    \n",
        "    for batch_num, (features, labels) in enumerate(dataset.take(num_batches)):\n",
        "        print(f\"\\nüì¶ Batch {batch_num + 1}:\")\n",
        "        print(\"‚îÄ\" * 50)\n",
        "        \n",
        "        # Structure des features\n",
        "        print(f\"üîß Structure des features:\")\n",
        "        for key, value in features.items():\n",
        "            print(f\"   ‚Ä¢ {key}: {value.shape} (dtype: {value.dtype})\")\n",
        "        \n",
        "        # Structure des labels\n",
        "        print(f\"üè∑Ô∏è Labels: {labels.shape} (dtype: {labels.dtype})\")\n",
        "        \n",
        "        # Exemple du premier √©l√©ment\n",
        "        print(f\"\\nüìù Premier exemple du batch:\")\n",
        "        input_ids_sample = features['input_ids'][0].numpy()\n",
        "        attention_mask_sample = features['attention_mask'][0].numpy()\n",
        "        label_sample = labels[0].numpy()\n",
        "        \n",
        "        print(f\"   üî¢ Input IDs (premiers 10): {input_ids_sample[:10]}\")\n",
        "        print(f\"   üëÅÔ∏è Attention mask (premiers 10): {attention_mask_sample[:10]}\")\n",
        "        print(f\"   üè∑Ô∏è Label: {label_sample} ({'Positif' if label_sample == 1 else 'N√©gatif'})\")\n",
        "        \n",
        "        # D√©codage de l'exemple\n",
        "        decoded_example = tokenizer.decode(input_ids_sample, skip_special_tokens=True)\n",
        "        print(f\"   üìÑ Texte d√©cod√©: '{decoded_example[:100]}{'...' if len(decoded_example) > 100 else ''}'\")\n",
        "        \n",
        "        # Statistiques du batch\n",
        "        print(f\"\\nüìä Statistiques du batch:\")\n",
        "        print(f\"   üìè Taille du batch: {len(labels)}\")\n",
        "        print(f\"   üòä Exemples positifs: {tf.reduce_sum(tf.cast(labels, tf.int32)).numpy()}\")\n",
        "        print(f\"   üòû Exemples n√©gatifs: {len(labels) - tf.reduce_sum(tf.cast(labels, tf.int32)).numpy()}\")\n",
        "        \n",
        "        # Analyse du padding\n",
        "        padding_ratios = []\n",
        "        for i in range(len(features['attention_mask'])):\n",
        "            mask = features['attention_mask'][i].numpy()\n",
        "            real_tokens = np.sum(mask)\n",
        "            padding_ratio = (len(mask) - real_tokens) / len(mask) * 100\n",
        "            padding_ratios.append(padding_ratio)\n",
        "        \n",
        "        avg_padding = np.mean(padding_ratios)\n",
        "        print(f\"   üìù Padding moyen: {avg_padding:.1f}%\")\n",
        "\n",
        "# Explorer les datasets\n",
        "explorer_dataset(train_dataset, \"Entra√Ænement\", 1)\n",
        "explorer_dataset(test_dataset, \"Validation\", 1)\n",
        "\n",
        "print(\"\\n‚úÖ Datasets TensorFlow pr√™ts pour l'entra√Ænement !\")\n",
        "print(\"\\nüéØ Prochaine √©tape: Construction du mod√®le DistilBERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Construction du Mod√®le avec DistilBERT\n",
        "\n",
        "Maintenant que nos donn√©es sont tokenis√©es, construisons notre mod√®le de classification bas√© sur DistilBERT pr√©-entra√Æn√©.\n",
        "\n",
        "### üéØ Qu'est-ce que DistilBERT ?\n",
        "\n",
        "**DistilBERT** est une version \"distill√©e\" de BERT :\n",
        "\n",
        "- **66 millions de param√®tres** (vs 110M pour BERT-base)\n",
        "- **6 couches Transformer** (vs 12 pour BERT)\n",
        "- **M√™me vocabulaire** que BERT (30k tokens)\n",
        "- **97% des performances** de BERT\n",
        "- **Pr√©-entra√Æn√©** sur de vastes corpus de texte anglais\n",
        "\n",
        "### üèóÔ∏è Architecture de notre Mod√®le\n",
        "\n",
        "Notre mod√®le aura cette structure :\n",
        "1. **DistilBERT pr√©-entra√Æn√©** (extraction de features)\n",
        "2. **Pooling du token [CLS]** (repr√©sentation globale)\n",
        "3. **Dropout** (r√©gularisation)\n",
        "4. **Dense layer** (classification binaire)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß† Chargement du mod√®le DistilBERT pr√©-entra√Æn√©\n",
        "print(\"üß† Chargement de DistilBERT pr√©-entra√Æn√©...\")\n",
        "print(\"(Cela peut prendre quelques minutes la premi√®re fois)\\n\")\n",
        "\n",
        "# Chargement depuis Hugging Face\n",
        "distilbert_model = TFAutoModel.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    return_dict=True  # Retour sous forme de dictionnaire\n",
        ")\n",
        "\n",
        "print(\"‚úÖ DistilBERT charg√© avec succ√®s !\")\n",
        "print(f\"üìä Nombre de param√®tres: {distilbert_model.num_parameters():,}\")\n",
        "print(f\"üèóÔ∏è Nombre de couches: 6 couches Transformer\")\n",
        "print(f\"üéØ Dimension des embeddings: 768\")\n",
        "print(f\"üî§ Vocabulaire: {tokenizer.vocab_size:,} tokens\")\n",
        "\n",
        "print(f\"\\nüéì Avantages de DistilBERT:\")\n",
        "print(f\"   ‚ö° 60% plus rapide que BERT\")\n",
        "print(f\"   üíæ 40% moins de m√©moire\")\n",
        "print(f\"   üéØ Performances quasi-identiques\")\n",
        "print(f\"   üì± Adapt√© aux applications en production\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üèóÔ∏è Construction du Mod√®le Complet\n",
        "\n",
        "Cr√©ons maintenant l'architecture compl√®te qui combine DistilBERT avec nos couches de classification :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üèóÔ∏è Fonction de cr√©ation du mod√®le complet\n",
        "def create_sentiment_model(distilbert_model, dropout_rate=0.3):\n",
        "    \"\"\"\n",
        "    Cr√©e un mod√®le de classification de sentiment bas√© sur DistilBERT\n",
        "    \n",
        "    Architecture :\n",
        "    1. DistilBERT pr√©-entra√Æn√© (couche de base)\n",
        "    2. Extraction du token [CLS] (repr√©sentation globale)\n",
        "    3. Dropout pour r√©gularisation\n",
        "    4. Dense layer pour classification binaire\n",
        "    \n",
        "    Args:\n",
        "        distilbert_model: Mod√®le DistilBERT pr√©-entra√Æn√©\n",
        "        dropout_rate: Taux de dropout (d√©faut: 0.3)\n",
        "    \n",
        "    Returns:\n",
        "        model: Mod√®le Keras complet\n",
        "        base_model: Mod√®le DistilBERT (pour le fine-tuning)\n",
        "    \"\"\"\n",
        "    print(\"üèóÔ∏è Construction de l'architecture compl√®te...\")\n",
        "    \n",
        "    # Couches d'entr√©e\n",
        "    input_ids = tf.keras.layers.Input(\n",
        "        shape=(MAX_LENGTH,), \n",
        "        dtype=tf.int32, \n",
        "        name='input_ids'\n",
        "    )\n",
        "    attention_mask = tf.keras.layers.Input(\n",
        "        shape=(MAX_LENGTH,), \n",
        "        dtype=tf.int32, \n",
        "        name='attention_mask'\n",
        "    )\n",
        "    \n",
        "    print(\"   ‚úÖ Couches d'entr√©e cr√©√©es\")\n",
        "    \n",
        "    # DistilBERT pr√©-entra√Æn√©\n",
        "    distilbert_output = distilbert_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    \n",
        "    # Extraction des embeddings de s√©quence\n",
        "    sequence_output = distilbert_output.last_hidden_state  # Shape: (batch_size, seq_len, 768)\n",
        "    print(\"   ‚úÖ DistilBERT int√©gr√©\")\n",
        "    \n",
        "    # Pooling : extraction du token [CLS] (premi√®re position)\n",
        "    cls_token = sequence_output[:, 0, :]  # Shape: (batch_size, 768)\n",
        "    print(\"   ‚úÖ Token [CLS] extrait\")\n",
        "    \n",
        "    # Couche de dropout pour la r√©gularisation\n",
        "    dropout_output = tf.keras.layers.Dropout(dropout_rate, name='dropout')(cls_token)\n",
        "    print(f\"   ‚úÖ Dropout ajout√© (rate={dropout_rate})\")\n",
        "    \n",
        "    # Couche de classification finale\n",
        "    predictions = tf.keras.layers.Dense(\n",
        "        1, \n",
        "        activation='sigmoid',  # Sigmoid pour classification binaire\n",
        "        name='classifier'\n",
        "    )(dropout_output)\n",
        "    print(\"   ‚úÖ Couche de classification ajout√©e\")\n",
        "    \n",
        "    # Cr√©ation du mod√®le final\n",
        "    model = tf.keras.Model(\n",
        "        inputs=[input_ids, attention_mask],\n",
        "        outputs=predictions,\n",
        "        name='DistilBERT_Sentiment_Classifier'\n",
        "    )\n",
        "    \n",
        "    print(\"\\n‚úÖ Mod√®le construit avec succ√®s !\")\n",
        "    \n",
        "    return model, distilbert_model\n",
        "\n",
        "# Construction du mod√®le\n",
        "model, base_model = create_sentiment_model(distilbert_model)\n",
        "\n",
        "# Informations sur le mod√®le\n",
        "total_params = sum(tf.size(var).numpy() for var in model.trainable_variables)\n",
        "print(f\"\\nüìä Statistiques du mod√®le:\")\n",
        "print(f\"   üî¢ Param√®tres totaux: {total_params:,}\")\n",
        "print(f\"   üéØ Type de classification: Binaire (sentiment)\")\n",
        "print(f\"   üìè Longueur d'entr√©e: {MAX_LENGTH} tokens\")\n",
        "print(f\"   üé≤ Dropout: 0.3 (30% des neurones d√©sactiv√©s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìã R√©sum√© de l'Architecture\n",
        "\n",
        "Visualisons l'architecture de notre mod√®le :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìã Affichage du r√©sum√© du mod√®le\n",
        "print(\"üìã Architecture d√©taill√©e du mod√®le:\")\n",
        "print(\"‚ïê\" * 80)\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nüéì Explication de l'architecture:\")\n",
        "print(\"   üì• Entr√©es:\")\n",
        "print(\"      ‚Ä¢ input_ids: Tokens num√©riques (batch_size, 256)\")\n",
        "print(\"      ‚Ä¢ attention_mask: Masque pour ignorer padding (batch_size, 256)\")\n",
        "print(\"   üß† DistilBERT:\")\n",
        "print(\"      ‚Ä¢ 6 couches Transformer\")\n",
        "print(\"      ‚Ä¢ Multi-head attention (12 t√™tes)\")\n",
        "print(\"      ‚Ä¢ Embeddings contextuels (768 dimensions)\")\n",
        "print(\"   üéØ Classification:\")\n",
        "print(\"      ‚Ä¢ Token [CLS] ‚Üí repr√©sentation globale\")\n",
        "print(\"      ‚Ä¢ Dropout ‚Üí r√©gularisation\")\n",
        "print(\"      ‚Ä¢ Dense(1) + Sigmoid ‚Üí probabilit√© sentiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé® Visualisation de l'Architecture\n",
        "\n",
        "Cr√©ons un diagramme pour mieux comprendre le flux de donn√©es :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé® Visualisation de l'architecture du mod√®le\n",
        "def visualiser_architecture_distilbert():\n",
        "    \"\"\"\n",
        "    Cr√©e un diagramme explicatif de l'architecture DistilBERT\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
        "    \n",
        "    # D√©finir les composants et leurs positions\n",
        "    components = [\n",
        "        {\"name\": \"Texte d'entr√©e\\n'This movie is amazing!'\\n(Critique de film)\", \"pos\": (1, 10), \"color\": \"lightblue\", \"frozen\": False},\n",
        "        {\"name\": \"Tokenizer DistilBERT\\n[CLS] this movie is amazing [SEP]\\n+ attention_mask\", \"pos\": (1, 8.5), \"color\": \"lightyellow\", \"frozen\": False},\n",
        "        {\"name\": \"Input IDs\\n[101, 2023, 3185, 2003, 6429, 102]\\n+ Attention Mask\", \"pos\": (1, 7), \"color\": \"lightgray\", \"frozen\": False},\n",
        "        {\"name\": \"DistilBERT\\n6 Couches Transformer\\nüîí PR√â-ENTRA√éN√â\", \"pos\": (1, 5.5), \"color\": \"lightcoral\", \"frozen\": True},\n",
        "        {\"name\": \"Sequence Output\\n(batch, 256, 768)\\nEmbeddings contextuels\", \"pos\": (1, 4), \"color\": \"lightgreen\", \"frozen\": True},\n",
        "        {\"name\": \"Token [CLS] Extraction\\n(batch, 768)\\nRepr√©sentation globale\", \"pos\": (1, 2.5), \"color\": \"lightyellow\", \"frozen\": False},\n",
        "        {\"name\": \"Dropout(0.3)\\nüé≤ R√©gularisation\", \"pos\": (1, 1), \"color\": \"lightpink\", \"frozen\": False},\n",
        "        {\"name\": \"Dense(1) + Sigmoid\\nüéØ Classification Binaire\\nProbabilit√© [0, 1]\", \"pos\": (1, -0.5), \"color\": \"lightsteelblue\", \"frozen\": False}\n",
        "    ]\n",
        "    \n",
        "    # Dessiner les composants\n",
        "    for i, comp in enumerate(components):\n",
        "        x, y = comp[\"pos\"]\n",
        "        \n",
        "        # Style diff√©rent pour les couches pr√©-entra√Æn√©es\n",
        "        if comp[\"frozen\"]:\n",
        "            bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=comp[\"color\"], \n",
        "                            edgecolor=\"red\", linewidth=2, linestyle=\"--\")\n",
        "        else:\n",
        "            bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=comp[\"color\"], \n",
        "                            edgecolor=\"black\", linewidth=1)\n",
        "        \n",
        "        ax.text(x, y, comp[\"name\"], ha=\"center\", va=\"center\", \n",
        "               fontsize=11, fontweight=\"bold\", bbox=bbox_props)\n",
        "        \n",
        "        # Fl√®ches entre composants\n",
        "        if i < len(components) - 1:\n",
        "            next_y = components[i+1][\"pos\"][1]\n",
        "            ax.annotate(\"\", xy=(x, next_y + 0.4), xytext=(x, y - 0.4),\n",
        "                       arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"darkblue\"))\n",
        "    \n",
        "    # Annotations explicatives\n",
        "    ax.text(4, 8, \"üí≠ Pr√©processing\\n‚Ä¢ Tokenisation WordPiece\\n‚Ä¢ Ajout tokens sp√©ciaux\\n‚Ä¢ Padding/Truncation\\n‚Ä¢ Attention mask\", \n",
        "           fontsize=10, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"wheat\", alpha=0.8))\n",
        "    \n",
        "    ax.text(4, 5, \"üß† DistilBERT Core\\n‚Ä¢ 6 couches Transformer\\n‚Ä¢ 12 t√™tes d'attention\\n‚Ä¢ 66M param√®tres\\n‚Ä¢ Pr√©-entra√Æn√© sur Wikipedia\", \n",
        "           fontsize=10, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcyan\", alpha=0.8))\n",
        "    \n",
        "    ax.text(4, 1.5, \"üéØ Classification\\n‚Ä¢ Token [CLS] = r√©sum√©\\n‚Ä¢ Dropout √©vite overfitting\\n‚Ä¢ Sigmoid ‚Üí probabilit√©\\n‚Ä¢ > 0.5 = Positif\", \n",
        "           fontsize=10, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lavender\", alpha=0.8))\n",
        "    \n",
        "    # Informations techniques sur le c√¥t√©\n",
        "    ax.text(7, 3, f\"‚öôÔ∏è Configuration Technique\\n\\nüìä Batch size: {BATCH_SIZE}\\nüìè Max length: {MAX_LENGTH}\\nüéØ Learning rate: {LEARNING_RATE}\\n‚è≥ Epochs: {EPOCHS}\\n\\nüíæ M√©moire GPU: ~2-4GB\\n‚ö° Vitesse: ~60% plus rapide que BERT\\nüéØ Performance: ~97% de BERT\", \n",
        "           fontsize=9, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.9))\n",
        "    \n",
        "    # Configuration des axes\n",
        "    ax.set_xlim(-0.5, 9)\n",
        "    ax.set_ylim(-2, 11)\n",
        "    ax.set_title(\"üß† Architecture DistilBERT pour Classification de Sentiment\", fontsize=16, fontweight=\"bold\", pad=20)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # L√©gende\n",
        "    legend_elements = [\n",
        "        plt.Rectangle((0, 0), 1, 1, facecolor='lightcoral', edgecolor='red', linestyle='--', label='Composants pr√©-entra√Æn√©s'),\n",
        "        plt.Rectangle((0, 0), 1, 1, facecolor='lightsteelblue', edgecolor='black', label='Nouvelles couches')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='upper left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"üé® Visualisation de l'architecture DistilBERT:\")\n",
        "visualiser_architecture_distilbert()\n",
        "\n",
        "print(\"\\nüí° Points cl√©s de l'architecture:\")\n",
        "print(\"   üß† DistilBERT: Mod√®le pr√©-entra√Æn√© riche en connaissances linguistiques\")\n",
        "print(\"   üéØ Token [CLS]: Repr√©sentation globale de tout le texte\")\n",
        "print(\"   üé≤ Dropout: √âvite le surapprentissage sur la nouvelle t√¢che\")\n",
        "print(\"   üìà Sigmoid: Sortie entre 0 et 1 (probabilit√© de sentiment positif)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Test du Mod√®le\n",
        "\n",
        "Testons que notre mod√®le fonctionne correctement avant l'entra√Ænement :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Test du mod√®le avant entra√Ænement\n",
        "def tester_modele_avant_entrainement(model, test_dataset):\n",
        "    \"\"\"\n",
        "    Teste que le mod√®le fonctionne correctement avant l'entra√Ænement\n",
        "    \"\"\"\n",
        "    print(\"üîß Test du mod√®le avant entra√Ænement...\\n\")\n",
        "    \n",
        "    # Prendre un petit batch pour le test\n",
        "    for batch_features, batch_labels in test_dataset.take(1):\n",
        "        print(f\"üì¶ Batch de test:\")\n",
        "        print(f\"   üìè Taille: {len(batch_labels)} exemples\")\n",
        "        print(f\"   üìä Input IDs shape: {batch_features['input_ids'].shape}\")\n",
        "        print(f\"   üëÅÔ∏è Attention mask shape: {batch_features['attention_mask'].shape}\")\n",
        "        print(f\"   üè∑Ô∏è Labels shape: {batch_labels.shape}\")\n",
        "        \n",
        "        # Forward pass (pr√©diction)\n",
        "        print(f\"\\nüîÆ Test de pr√©diction...\")\n",
        "        predictions = model([\n",
        "            batch_features['input_ids'],\n",
        "            batch_features['attention_mask']\n",
        "        ], training=False)\n",
        "        \n",
        "        print(f\"‚úÖ Pr√©dictions g√©n√©r√©es !\")\n",
        "        print(f\"   üìä Shape des pr√©dictions: {predictions.shape}\")\n",
        "        print(f\"   üìà Valeurs min/max: {tf.reduce_min(predictions):.4f} / {tf.reduce_max(predictions):.4f}\")\n",
        "        print(f\"   üéØ Moyenne: {tf.reduce_mean(predictions):.4f}\")\n",
        "        \n",
        "        # Analyse des premi√®res pr√©dictions\n",
        "        print(f\"\\nüìù Premi√®res pr√©dictions (avant entra√Ænement):\")\n",
        "        for i in range(min(5, len(predictions))):\n",
        "            pred_prob = predictions[i].numpy()[0]\n",
        "            true_label = batch_labels[i].numpy()\n",
        "            pred_sentiment = \"Positif\" if pred_prob > 0.5 else \"N√©gatif\"\n",
        "            true_sentiment = \"Positif\" if true_label == 1 else \"N√©gatif\"\n",
        "            \n",
        "            print(f\"   Exemple {i+1}: Pr√©dit={pred_sentiment} ({pred_prob:.3f}), Vrai={true_sentiment}\")\n",
        "        \n",
        "        break\n",
        "    \n",
        "    print(f\"\\nüí° Observations:\")\n",
        "    print(f\"   üé≤ Pr√©dictions al√©atoires (mod√®le non entra√Æn√©)\")\n",
        "    print(f\"   üìä Valeurs entre 0 et 1 (sigmoid fonctionne)\")\n",
        "    print(f\"   ‚úÖ Architecture compatible avec nos donn√©es\")\n",
        "    print(f\"   üöÄ Pr√™t pour l'entra√Ænement !\")\n",
        "\n",
        "# Tester le mod√®le\n",
        "tester_modele_avant_entrainement(model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Compilation du Mod√®le\n",
        "\n",
        "Maintenant que notre architecture est construite, configurons l'entra√Ænement :\n",
        "\n",
        "### üéØ Configuration des Hyperparam√®tres\n",
        "\n",
        "Pour le fine-tuning de DistilBERT, nous utiliserons :\n",
        "\n",
        "- **Loss Function** : `BinaryCrossentropy` (classification binaire)\n",
        "- **Optimiseur** : `Adam` avec learning rate adaptatif\n",
        "- **M√©triques** : `Accuracy` et `Precision/Recall`\n",
        "- **Learning Rate** : Plus faible pour pr√©server les poids pr√©-entra√Æn√©s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è Configuration de l'optimiseur et des m√©triques\n",
        "print(\"‚öôÔ∏è Configuration de l'entra√Ænement...\\n\")\n",
        "\n",
        "# Optimiseur Adam avec learning rate adapt√© au fine-tuning\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=LEARNING_RATE,  # 2e-5 - plus faible pour pr√©server les poids\n",
        "    epsilon=1e-08,               # Stabilit√© num√©rique\n",
        "    clipnorm=1.0                 # Gradient clipping\n",
        ")\n",
        "\n",
        "print(f\"üéØ Optimiseur configur√©:\")\n",
        "print(f\"   üìä Type: Adam\")\n",
        "print(f\"   üìà Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   ‚úÇÔ∏è Gradient clipping: 1.0\")\n",
        "print(f\"   üéØ Epsilon: 1e-08\")\n",
        "\n",
        "# Loss function pour classification binaire\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "print(f\"\\nüìâ Loss function:\")\n",
        "print(f\"   üéØ Type: Binary Crossentropy\")\n",
        "print(f\"   üìä From logits: False (on utilise sigmoid)\")\n",
        "\n",
        "# M√©triques de suivi\n",
        "metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall')\n",
        "]\n",
        "\n",
        "print(f\"\\nüìä M√©triques de suivi:\")\n",
        "for metric in metrics:\n",
        "    print(f\"   ‚Ä¢ {metric.name}\")\n",
        "\n",
        "print(f\"\\nüí° Pourquoi ces choix ?\")\n",
        "print(f\"   üìà Learning rate faible: Pr√©server les connaissances pr√©-entra√Æn√©es\")\n",
        "print(f\"   ‚úÇÔ∏è Gradient clipping: √âviter l'explosion des gradients\")\n",
        "print(f\"   üìä Multiple m√©triques: Vue compl√®te des performances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Compilation du Mod√®le\n",
        "\n",
        "Compilons maintenant notre mod√®le avec ces param√®tres :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Compilation du mod√®le\n",
        "print(\"üîß Compilation du mod√®le...\\n\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Mod√®le compil√© avec succ√®s !\")\n",
        "\n",
        "# V√©rification de la configuration\n",
        "print(f\"\\nüìã Configuration finale:\")\n",
        "print(f\"   üéØ T√¢che: Classification binaire de sentiment\")\n",
        "print(f\"   üß† Architecture: DistilBERT + Classification head\")\n",
        "print(f\"   üìä Param√®tres: {sum(tf.size(var).numpy() for var in model.trainable_variables):,}\")\n",
        "print(f\"   ‚ö° Optimiseur: {optimizer.__class__.__name__}\")\n",
        "print(f\"   üìâ Loss: {loss_fn.__class__.__name__}\")\n",
        "print(f\"   üìà M√©triques: {[m.name for m in metrics]}\")\n",
        "\n",
        "print(f\"\\nüöÄ Mod√®le pr√™t pour l'entra√Ænement !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìû Configuration des Callbacks\n",
        "\n",
        "Les callbacks nous permettent de surveiller et contr√¥ler l'entra√Ænement :\n",
        "\n",
        "### üéØ Callbacks Utiles pour le Fine-tuning\n",
        "\n",
        "- **EarlyStopping** : Arr√™t automatique si pas d'am√©lioration\n",
        "- **ReduceLROnPlateau** : R√©duction du learning rate si stagnation\n",
        "- **ModelCheckpoint** : Sauvegarde du meilleur mod√®le\n",
        "- **CSVLogger** : Enregistrement des m√©triques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìû Configuration des callbacks pour l'entra√Ænement\n",
        "print(\"üìû Configuration des callbacks...\\n\")\n",
        "\n",
        "# Early Stopping - arr√™t si pas d'am√©lioration\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',     # M√©trique √† surveiller\n",
        "    patience=3,                 # Nombre d'epochs sans am√©lioration\n",
        "    restore_best_weights=True,  # Restaurer les meilleurs poids\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"üõë Early Stopping configur√©:\")\n",
        "print(f\"   üìä Surveille: validation accuracy\")\n",
        "print(f\"   ‚è≥ Patience: 3 epochs\")\n",
        "print(f\"   üîÑ Restaure meilleurs poids: Oui\")\n",
        "\n",
        "# Reduce Learning Rate - diminution si stagnation\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',     # M√©trique √† surveiller\n",
        "    factor=0.5,             # Facteur de r√©duction (LR = LR * 0.5)\n",
        "    patience=2,             # Patience avant r√©duction\n",
        "    min_lr=1e-7,           # Learning rate minimum\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\nüìâ Reduce LR configur√©:\")\n",
        "print(f\"   üìä Surveille: validation loss\")\n",
        "print(f\"   üìà Facteur: 0.5 (divise par 2)\")\n",
        "print(f\"   ‚è≥ Patience: 2 epochs\")\n",
        "print(f\"   üîª LR minimum: 1e-7\")\n",
        "\n",
        "# Model Checkpoint - sauvegarde du meilleur mod√®le\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_distilbert_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"\\nüíæ Model Checkpoint configur√©:\")\n",
        "print(f\"   üìÅ Fichier: best_distilbert_model.h5\")\n",
        "print(f\"   üìä Surveille: validation accuracy\")\n",
        "print(f\"   üíæ Sauve: Mod√®le complet\")\n",
        "\n",
        "# CSV Logger - enregistrement des m√©triques\n",
        "csv_logger = tf.keras.callbacks.CSVLogger(\n",
        "    'training_log_distilbert.csv',\n",
        "    append=True\n",
        ")\n",
        "\n",
        "print(f\"\\nüìù CSV Logger configur√©:\")\n",
        "print(f\"   üìÅ Fichier: training_log_distilbert.csv\")\n",
        "print(f\"   üìä Contenu: Toutes les m√©triques par epoch\")\n",
        "\n",
        "# Liste compl√®te des callbacks\n",
        "callbacks = [early_stopping, reduce_lr, checkpoint, csv_logger]\n",
        "\n",
        "print(f\"\\n‚úÖ {len(callbacks)} callbacks configur√©s:\")\n",
        "for i, cb in enumerate(callbacks, 1):\n",
        "    print(f\"   {i}. {cb.__class__.__name__}\")\n",
        "\n",
        "print(f\"\\nüí° Avantages des callbacks:\")\n",
        "print(f\"   üõë √âvite le surapprentissage (Early Stopping)\")\n",
        "print(f\"   üìà Optimise l'apprentissage (ReduceLR)\")\n",
        "print(f\"   üíæ Sauvegarde automatique (Checkpoint)\")\n",
        "print(f\"   üìä Tra√ßabilit√© compl√®te (CSV Logger)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Lancement de l'Entra√Ænement (Fine-tuning)\n",
        "\n",
        "C'est le moment de d√©marrer le fine-tuning de DistilBERT !\n",
        "\n",
        "### üéØ Strat√©gie de Fine-tuning\n",
        "\n",
        "Nous utilisons une approche **End-to-End** :\n",
        "- Toutes les couches sont entra√Ænables d√®s le d√©but\n",
        "- Learning rate faible pour pr√©server les connaissances\n",
        "- R√©gularisation avec dropout\n",
        "- Surveillance avec callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Pr√©paration au lancement de l'entra√Ænement\n",
        "print(\"üöÄ Pr√©paration de l'entra√Ænement...\\n\")\n",
        "\n",
        "# V√©rification des param√®tres d'entra√Ænement\n",
        "print(\"üìä Param√®tres d'entra√Ænement:\")\n",
        "print(f\"   üìà Epochs: {EPOCHS}\")\n",
        "print(f\"   üì¶ Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   üìè Sequence length: {MAX_LENGTH}\")\n",
        "print(f\"   üéØ Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# Calcul du nombre de steps par epoch\n",
        "steps_per_epoch = len(list(train_dataset.as_numpy_iterator()))\n",
        "validation_steps = len(list(test_dataset.as_numpy_iterator()))\n",
        "\n",
        "\n",
        "print(f\"\\nüìä Configuration des donn√©es:\")\n",
        "print(f\"   üèãÔ∏è Steps par epoch: {steps_per_epoch}\")\n",
        "print(f\"   ‚úÖ Steps de validation: {validation_steps}\")\n",
        "print(f\"   ‚è±Ô∏è Temps estim√©: ~{EPOCHS * 3}-{EPOCHS * 5} minutes\")\n",
        "\n",
        "# Estimation m√©moire GPU\n",
        "print(f\"\\nüíæ Utilisation m√©moire estim√©e:\")\n",
        "print(f\"   üñ•Ô∏è GPU: ~2-4 GB\")\n",
        "print(f\"   üß† Param√®tres: {sum(tf.size(var).numpy() for var in model.trainable_variables):,}\")\n",
        "\n",
        "print(f\"\\nüéØ Objectif: Classifier le sentiment (Positif/N√©gatif)\")\n",
        "print(f\"üìä M√©trique principale: Validation Accuracy\")\n",
        "print(f\"\\nüöÄ Lancement de l'entra√Ænement !\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Entra√Ænement du mod√®le\n",
        "print(\"üî• D√âBUT DE L'ENTRA√éNEMENT\\n\")\n",
        "\n",
        "# Enregistrer l'heure de d√©but\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Entra√Ænement avec fit()\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Calcul du temps d'entra√Ænement\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nüéâ ENTRA√éNEMENT TERMIN√â !\")\n",
        "print(f\"‚è±Ô∏è Temps total: {training_time/60:.1f} minutes\")\n",
        "print(f\"‚ö° Temps par epoch: {training_time/EPOCHS:.1f} secondes\")\n",
        "\n",
        "# Sauvegarde de l'historique\n",
        "import pickle\n",
        "with open('training_history_distilbert.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "print(f\"\\nüíæ Historique sauvegard√© dans 'training_history_distilbert.pkl'\")\n",
        "print(f\"üìä Mod√®le sauvegard√© automatiquement par ModelCheckpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä R√©sum√© de l'Entra√Ænement\n",
        "\n",
        "Analysons rapidement les r√©sultats de l'entra√Ænement :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Analyse rapide des r√©sultats d'entra√Ænement\n",
        "print(\"üìä R√âSUM√â DE L'ENTRA√éNEMENT\\n\")\n",
        "\n",
        "# R√©cup√©ration des m√©triques finales\n",
        "final_train_accuracy = history.history['accuracy'][-1]\n",
        "final_val_accuracy = history.history['val_accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"üéØ Performances finales:\")\n",
        "print(f\"   üìà Accuracy entra√Ænement: {final_train_accuracy:.4f} ({final_train_accuracy*100:.2f}%)\")\n",
        "print(f\"   ‚úÖ Accuracy validation: {final_val_accuracy:.4f} ({final_val_accuracy*100:.2f}%)\")\n",
        "print(f\"   üìâ Loss entra√Ænement: {final_train_loss:.4f}\")\n",
        "print(f\"   üìä Loss validation: {final_val_loss:.4f}\")\n",
        "\n",
        "# Analyse de l'overfitting\n",
        "overfitting_gap = final_train_accuracy - final_val_accuracy\n",
        "print(f\"\\nüîç Analyse:\")\n",
        "if overfitting_gap < 0.05:\n",
        "    print(f\"   ‚úÖ Bon √©quilibre (gap: {overfitting_gap:.4f})\")\n",
        "elif overfitting_gap < 0.10:\n",
        "    print(f\"   ‚ö†Ô∏è L√©ger overfitting (gap: {overfitting_gap:.4f})\")\n",
        "else:\n",
        "    print(f\"   üö® Overfitting d√©tect√© (gap: {overfitting_gap:.4f})\")\n",
        "\n",
        "# Meilleure √©poch\n",
        "best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "\n",
        "print(f\"\\nüèÜ Meilleure performance:\")\n",
        "print(f\"   üìä √âpoque: {best_epoch}/{EPOCHS}\")\n",
        "print(f\"   üéØ Validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "\n",
        "# √âvaluation de la performance\n",
        "if best_val_acc > 0.90:\n",
        "    print(f\"   üåü Excellente performance !\")\n",
        "elif best_val_acc > 0.85:\n",
        "    print(f\"   üëç Tr√®s bonne performance !\")\n",
        "elif best_val_acc > 0.80:\n",
        "    print(f\"   ‚úÖ Bonne performance\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Performance √† am√©liorer\")\n",
        "\n",
        "print(f\"\\nüí° Prochaine √©tape: Visualisation d√©taill√©e des courbes d'apprentissage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualisation des Courbes d'Entra√Ænement\n",
        "\n",
        "Analysons en d√©tail l'√©volution de notre mod√®le pendant l'entra√Ænement.\n",
        "\n",
        "### üéØ M√©triques √† Analyser\n",
        "\n",
        "- **Loss** : √âvolution de l'erreur (diminution souhait√©e)\n",
        "- **Accuracy** : Pr√©cision de classification (augmentation souhait√©e)\n",
        "- **Precision/Recall** : M√©triques d√©taill√©es\n",
        "- **√âcart train/validation** : D√©tection du surapprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà Fonction de visualisation des courbes d'entra√Ænement\n",
        "def visualiser_courbes_entrainement(history):\n",
        "    \"\"\"\n",
        "    Cr√©e des graphiques d√©taill√©s des m√©triques d'entra√Ænement\n",
        "    \"\"\"\n",
        "    print(\"üìà Cr√©ation des visualisations...\\n\")\n",
        "    \n",
        "    # Configuration de la figure\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('üìä √âvolution des M√©triques - Fine-tuning DistilBERT', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Couleurs pour les graphiques\n",
        "    colors = {'train': '#2E86AB', 'val': '#A23B72'}\n",
        "    \n",
        "    # 1. Loss (Perte)\n",
        "    ax1 = axes[0, 0]\n",
        "    epochs_range = range(1, len(history.history['loss']) + 1)\n",
        "    \n",
        "    ax1.plot(epochs_range, history.history['loss'], \n",
        "             color=colors['train'], marker='o', linewidth=2, label='Entra√Ænement')\n",
        "    ax1.plot(epochs_range, history.history['val_loss'], \n",
        "             color=colors['val'], marker='s', linewidth=2, label='Validation')\n",
        "    \n",
        "    ax1.set_title('üìâ √âvolution de la Loss', fontweight='bold')\n",
        "    ax1.set_xlabel('√âpoque')\n",
        "    ax1.set_ylabel('Binary Crossentropy Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Annotation du minimum\n",
        "    min_val_loss = min(history.history['val_loss'])\n",
        "    min_epoch = history.history['val_loss'].index(min_val_loss) + 1\n",
        "    ax1.annotate(f'Min: {min_val_loss:.4f}\\n(√âp. {min_epoch})', \n",
        "                xy=(min_epoch, min_val_loss), xytext=(min_epoch+0.5, min_val_loss+0.01),\n",
        "                arrowprops=dict(arrowstyle='->', color='red'), fontsize=9)\n",
        "    \n",
        "    # 2. Accuracy (Pr√©cision)\n",
        "    ax2 = axes[0, 1]\n",
        "    \n",
        "    ax2.plot(epochs_range, [acc*100 for acc in history.history['accuracy']], \n",
        "             color=colors['train'], marker='o', linewidth=2, label='Entra√Ænement')\n",
        "    ax2.plot(epochs_range, [acc*100 for acc in history.history['val_accuracy']], \n",
        "             color=colors['val'], marker='s', linewidth=2, label='Validation')\n",
        "    \n",
        "    ax2.set_title('üìä √âvolution de l\\'Accuracy', fontweight='bold')\n",
        "    ax2.set_xlabel('√âpoque')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Annotation du maximum\n",
        "    max_val_acc = max(history.history['val_accuracy'])\n",
        "    max_epoch = history.history['val_accuracy'].index(max_val_acc) + 1\n",
        "    ax2.annotate(f'Max: {max_val_acc*100:.2f}%\\n(√âp. {max_epoch})', \n",
        "                xy=(max_epoch, max_val_acc*100), \n",
        "                xytext=(max_epoch+0.5, max_val_acc*100-2),\n",
        "                arrowprops=dict(arrowstyle='->', color='green'), fontsize=9)\n",
        "    \n",
        "    # 3. Precision\n",
        "    ax3 = axes[1, 0]\n",
        "    \n",
        "    ax3.plot(epochs_range, [p*100 for p in history.history['precision']], \n",
        "             color=colors['train'], marker='o', linewidth=2, label='Entra√Ænement')\n",
        "    ax3.plot(epochs_range, [p*100 for p in history.history['val_precision']], \n",
        "             color=colors['val'], marker='s', linewidth=2, label='Validation')\n",
        "    \n",
        "    ax3.set_title('üéØ √âvolution de la Precision', fontweight='bold')\n",
        "    ax3.set_xlabel('√âpoque')\n",
        "    ax3.set_ylabel('Precision (%)')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Recall\n",
        "    ax4 = axes[1, 1]\n",
        "    \n",
        "    ax4.plot(epochs_range, [r*100 for r in history.history['recall']], \n",
        "             color=colors['train'], marker='o', linewidth=2, label='Entra√Ænement')\n",
        "    ax4.plot(epochs_range, [r*100 for r in history.history['val_recall']], \n",
        "             color=colors['val'], marker='s', linewidth=2, label='Validation')\n",
        "    \n",
        "    ax4.set_title('üîç √âvolution du Recall', fontweight='bold')\n",
        "    ax4.set_xlabel('√âpoque')\n",
        "    ax4.set_ylabel('Recall (%)')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ Graphiques des m√©triques g√©n√©r√©s !\")\n",
        "\n",
        "# Cr√©ation des visualisations\n",
        "visualiser_courbes_entrainement(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Analyse D√©taill√©e des R√©sultats\n",
        "\n",
        "Cr√©ons un tableau r√©capitulatif des performances :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Analyse statistique d√©taill√©e\n",
        "def analyser_performances_entrainement(history):\n",
        "    \"\"\"\n",
        "    Analyse compl√®te des performances d'entra√Ænement\n",
        "    \"\"\"\n",
        "    print(\"üìä ANALYSE D√âTAILL√âE DES PERFORMANCES\\n\")\n",
        "    \n",
        "    # Extraction des m√©triques\n",
        "    metrics = {\n",
        "        'Loss': {\n",
        "            'train': history.history['loss'],\n",
        "            'val': history.history['val_loss']\n",
        "        },\n",
        "        'Accuracy': {\n",
        "            'train': history.history['accuracy'],\n",
        "            'val': history.history['val_accuracy']\n",
        "        },\n",
        "        'Precision': {\n",
        "            'train': history.history['precision'],\n",
        "            'val': history.history['val_precision']\n",
        "        },\n",
        "        'Recall': {\n",
        "            'train': history.history['recall'],\n",
        "            'val': history.history['val_recall']\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Cr√©ation du tableau r√©capitulatif\n",
        "    import pandas as pd\n",
        "    \n",
        "    results = []\n",
        "    for metric_name, values in metrics.items():\n",
        "        train_final = values['train'][-1]\n",
        "        val_final = values['val'][-1]\n",
        "        \n",
        "        if metric_name == 'Loss':\n",
        "            train_best = min(values['train'])\n",
        "            val_best = min(values['val'])\n",
        "            best_epoch = values['val'].index(val_best) + 1\n",
        "        else:\n",
        "            train_best = max(values['train'])\n",
        "            val_best = max(values['val'])\n",
        "            best_epoch = values['val'].index(val_best) + 1\n",
        "        \n",
        "        results.append({\n",
        "            'M√©trique': metric_name,\n",
        "            'Train Final': f\"{train_final:.4f}\",\n",
        "            'Val Final': f\"{val_final:.4f}\",\n",
        "            'Meilleur Val': f\"{val_best:.4f}\",\n",
        "            '√âpoque Opt.': best_epoch,\n",
        "            'Gap Train-Val': f\"{abs(train_final - val_final):.4f}\"\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"üìã Tableau r√©capitulatif:\")\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    # Analyse de convergence\n",
        "    print(f\"\\nüîç Analyse de convergence:\")\n",
        "    \n",
        "    val_acc = metrics['Accuracy']['val']\n",
        "    if len(val_acc) >= 3:\n",
        "        last_3_epochs = val_acc[-3:]\n",
        "        is_stable = max(last_3_epochs) - min(last_3_epochs) < 0.01\n",
        "        \n",
        "        if is_stable:\n",
        "            print(f\"   ‚úÖ Convergence stable (variation < 1%)\")\n",
        "        else:\n",
        "            print(f\"   üìà Mod√®le encore en apprentissage\")\n",
        "    \n",
        "    # D√©tection d'overfitting\n",
        "    train_acc_final = metrics['Accuracy']['train'][-1]\n",
        "    val_acc_final = metrics['Accuracy']['val'][-1]\n",
        "    overfitting_gap = train_acc_final - val_acc_final\n",
        "    \n",
        "    print(f\"\\nüéØ Analyse de l'overfitting:\")\n",
        "    if overfitting_gap < 0.02:\n",
        "        print(f\"   ‚úÖ Pas d'overfitting (gap: {overfitting_gap:.4f})\")\n",
        "    elif overfitting_gap < 0.05:\n",
        "        print(f\"   ‚ö†Ô∏è Overfitting l√©ger (gap: {overfitting_gap:.4f})\")\n",
        "    else:\n",
        "        print(f\"   üö® Overfitting significatif (gap: {overfitting_gap:.4f})\")\n",
        "    \n",
        "    # F1-Score approximatif\n",
        "    precision_final = metrics['Precision']['val'][-1]\n",
        "    recall_final = metrics['Recall']['val'][-1]\n",
        "    f1_score = 2 * (precision_final * recall_final) / (precision_final + recall_final)\n",
        "    \n",
        "    print(f\"\\nüèÜ Score F1 estim√©: {f1_score:.4f} ({f1_score*100:.2f}%)\")\n",
        "    \n",
        "    # Recommandations\n",
        "    print(f\"\\nüí° Recommandations:\")\n",
        "    if val_acc_final > 0.90:\n",
        "        print(f\"   üåü Excellente performance ! Mod√®le pr√™t pour la production\")\n",
        "    elif val_acc_final > 0.85:\n",
        "        print(f\"   üëç Tr√®s bonne performance, peut √™tre d√©ploy√©\")\n",
        "    elif val_acc_final > 0.80:\n",
        "        print(f\"   ‚úÖ Performance satisfaisante\")\n",
        "        if overfitting_gap > 0.05:\n",
        "            print(f\"   üìù Conseil: Augmenter la r√©gularisation (dropout)\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Performance √† am√©liorer\")\n",
        "        print(f\"   üìù Conseils: Plus d'epochs, ajuster learning rate, ou plus de donn√©es\")\n",
        "\n",
        "# Lancer l'analyse\n",
        "analyser_performances_entrainement(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ √âvaluation des Performances\n",
        "\n",
        "Maintenant que notre mod√®le est entra√Æn√©, √©valuons ses performances sur l'ensemble de test.\n",
        "\n",
        "### üìä M√©triques d'√âvaluation\n",
        "\n",
        "Pour une classification binaire de sentiment, nous analyserons :\n",
        "\n",
        "- **Accuracy** : Pourcentage de pr√©dictions correctes\n",
        "- **Precision** : Qualit√© des pr√©dictions positives\n",
        "- **Recall** : Capacit√© √† d√©tecter les vrais positifs\n",
        "- **F1-Score** : Moyenne harmonique de precision et recall\n",
        "- **Matrice de confusion** : R√©partition d√©taill√©e des erreurs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ √âvaluation sur l'ensemble de test\n",
        "print(\"üéØ √âVALUATION SUR L'ENSEMBLE DE TEST\\n\")\n",
        "\n",
        "# Chargement du meilleur mod√®le (sauvegard√© par ModelCheckpoint)\n",
        "print(\"üìÇ Chargement du meilleur mod√®le...\")\n",
        "try:\n",
        "    best_model = tf.keras.models.load_model('best_distilbert_model.h5')\n",
        "    print(\"‚úÖ Meilleur mod√®le charg√© depuis ModelCheckpoint\")\n",
        "except:\n",
        "    best_model = model  # Utiliser le mod√®le actuel si pas de sauvegarde\n",
        "    print(\"‚ö†Ô∏è Utilisation du mod√®le actuel (pas de checkpoint trouv√©)\")\n",
        "\n",
        "# √âvaluation compl√®te\n",
        "print(f\"\\nüî¨ √âvaluation en cours...\")\n",
        "test_results = best_model.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "# R√©cup√©ration des m√©triques\n",
        "test_loss = test_results[0]\n",
        "test_accuracy = test_results[1]\n",
        "test_precision = test_results[2]\n",
        "test_recall = test_results[3]\n",
        "\n",
        "# Calcul du F1-Score\n",
        "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
        "\n",
        "print(f\"\\nüìä R√âSULTATS SUR L'ENSEMBLE DE TEST:\")\n",
        "print(f\"‚ïê\" * 50)\n",
        "print(f\"üéØ Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"üéØ Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
        "print(f\"üéØ Recall:    {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
        "print(f\"üèÜ F1-Score:  {f1_score:.4f} ({f1_score*100:.2f}%)\")\n",
        "print(f\"üìâ Loss:      {test_loss:.4f}\")\n",
        "print(f\"‚ïê\" * 50)\n",
        "\n",
        "# Interpr√©tation des r√©sultats\n",
        "print(f\"\\nüí≠ Interpr√©tation:\")\n",
        "if test_accuracy > 0.90:\n",
        "    print(f\"   üåü Excellente performance ! Mod√®le pr√™t pour la production\")\n",
        "elif test_accuracy > 0.85:\n",
        "    print(f\"   üéâ Tr√®s bonne performance ! Mod√®le fiable\")\n",
        "elif test_accuracy > 0.80:\n",
        "    print(f\"   ‚úÖ Bonne performance, utilisable en pratique\")\n",
        "elif test_accuracy > 0.75:\n",
        "    print(f\"   üëç Performance correcte, peut √™tre am√©lior√©e\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Performance insuffisante, n√©cessite optimisation\")\n",
        "\n",
        "# Analyse √©quilibre Precision/Recall\n",
        "balance = abs(test_precision - test_recall)\n",
        "if balance < 0.05:\n",
        "    print(f\"   ‚öñÔ∏è Bon √©quilibre Precision/Recall (diff: {balance:.3f})\")\n",
        "else:\n",
        "    print(f\"   ‚öñÔ∏è D√©s√©quilibre Precision/Recall (diff: {balance:.3f})\")\n",
        "    if test_precision > test_recall:\n",
        "        print(f\"      ‚Üí Mod√®le conservateur (peu de faux positifs)\")\n",
        "    else:\n",
        "        print(f\"      ‚Üí Mod√®le permissif (peu de faux n√©gatifs)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Matrice de Confusion\n",
        "\n",
        "Analysons en d√©tail les types d'erreurs commises par notre mod√®le :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîç G√©n√©ration des pr√©dictions pour la matrice de confusion\n",
        "print(\"üîç G√©n√©ration des pr√©dictions pour analyse d√©taill√©e...\\n\")\n",
        "\n",
        "# R√©cup√©ration des vraies √©tiquettes et pr√©dictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_pred_proba = []\n",
        "\n",
        "print(\"üìä Collecte des pr√©dictions...\")\n",
        "for batch_features, batch_labels in test_dataset:\n",
        "    # Pr√©dictions\n",
        "    predictions = best_model([\n",
        "        batch_features['input_ids'],\n",
        "        batch_features['attention_mask']\n",
        "    ], training=False)\n",
        "    \n",
        "    # Conversion en listes\n",
        "    y_true.extend(batch_labels.numpy())\n",
        "    y_pred_proba.extend(predictions.numpy().flatten())\n",
        "    y_pred.extend((predictions.numpy() > 0.5).astype(int).flatten())\n",
        "\n",
        "print(f\"‚úÖ {len(y_true)} pr√©dictions collect√©es\")\n",
        "\n",
        "# Conversion en arrays numpy\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_proba = np.array(y_pred_proba)\n",
        "\n",
        "print(f\"üìä Distribution des vraies √©tiquettes:\")\n",
        "print(f\"   N√©gatives (0): {np.sum(y_true == 0)} ({np.sum(y_true == 0)/len(y_true)*100:.1f}%)\")\n",
        "print(f\"   Positives (1): {np.sum(y_true == 1)} ({np.sum(y_true == 1)/len(y_true)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüéØ Distribution des pr√©dictions:\")\n",
        "print(f\"   N√©gatives (0): {np.sum(y_pred == 0)} ({np.sum(y_pred == 0)/len(y_pred)*100:.1f}%)\")\n",
        "print(f\"   Positives (1): {np.sum(y_pred == 1)} ({np.sum(y_pred == 1)/len(y_pred)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìà Cr√©ation de la matrice de confusion et visualisations\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "def visualiser_matrice_confusion(y_true, y_pred, y_pred_proba):\n",
        "    \"\"\"\n",
        "    Cr√©e une visualisation compl√®te de la matrice de confusion\n",
        "    \"\"\"\n",
        "    print(\"üìà Cr√©ation de la matrice de confusion...\\n\")\n",
        "    \n",
        "    # Calcul de la matrice\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Configuration de la figure\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    # 1. Matrice de confusion\n",
        "    ax1 = axes[0]\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['N√©gatif', 'Positif'],\n",
        "                yticklabels=['N√©gatif', 'Positif'], ax=ax1)\n",
        "    ax1.set_title('üîç Matrice de Confusion', fontweight='bold')\n",
        "    ax1.set_xlabel('Pr√©diction')\n",
        "    ax1.set_ylabel('V√©rit√©')\n",
        "    \n",
        "    # Ajout des pourcentages\n",
        "    total = cm.sum()\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            percentage = cm[i, j] / total * 100\n",
        "            ax1.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
        "                    ha='center', va='center', fontsize=10, color='gray')\n",
        "    \n",
        "    # 2. Distribution des probabilit√©s pr√©dites\n",
        "    ax2 = axes[1]\n",
        "    \n",
        "    # S√©parer par classe r√©elle\n",
        "    neg_probs = y_pred_proba[y_true == 0]\n",
        "    pos_probs = y_pred_proba[y_true == 1]\n",
        "    \n",
        "    ax2.hist(neg_probs, bins=30, alpha=0.7, label='Vrais N√©gatifs', color='lightcoral')\n",
        "    ax2.hist(pos_probs, bins=30, alpha=0.7, label='Vrais Positifs', color='lightblue')\n",
        "    ax2.axvline(x=0.5, color='red', linestyle='--', label='Seuil (0.5)')\n",
        "    \n",
        "    ax2.set_title('üìä Distribution des Probabilit√©s', fontweight='bold')\n",
        "    ax2.set_xlabel('Probabilit√© pr√©dite')\n",
        "    ax2.set_ylabel('Fr√©quence')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Analyse des erreurs\n",
        "    ax3 = axes[2]\n",
        "    \n",
        "    # Types d'erreurs\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    categories = ['Vrais\\nN√©gatifs', 'Faux\\nPositifs', 'Faux\\nN√©gatifs', 'Vrais\\nPositifs']\n",
        "    values = [tn, fp, fn, tp]\n",
        "    colors = ['lightgreen', 'lightcoral', 'lightsalmon', 'lightblue']\n",
        "    \n",
        "    bars = ax3.bar(categories, values, color=colors)\n",
        "    ax3.set_title('üìä R√©partition des Pr√©dictions', fontweight='bold')\n",
        "    ax3.set_ylabel('Nombre d\\'exemples')\n",
        "    \n",
        "    # Ajout des valeurs sur les barres\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                f'{value}\\n({value/total*100:.1f}%)', \n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyse textuelle\n",
        "    print(f\"üìã Analyse d√©taill√©e de la matrice:\")\n",
        "    print(f\"   ‚úÖ Vrais N√©gatifs (TN):  {tn:4d} ({tn/total*100:.1f}%)\")\n",
        "    print(f\"   ‚ùå Faux Positifs (FP):   {fp:4d} ({fp/total*100:.1f}%) - Erreur Type I\")\n",
        "    print(f\"   ‚ùå Faux N√©gatifs (FN):   {fn:4d} ({fn/total*100:.1f}%) - Erreur Type II\")\n",
        "    print(f\"   ‚úÖ Vrais Positifs (TP):  {tp:4d} ({tp/total*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nüéØ Interpr√©tation des erreurs:\")\n",
        "    if fp > fn:\n",
        "        print(f\"   üìä Plus de Faux Positifs: Le mod√®le tend √† sur-classifier comme positif\")\n",
        "        print(f\"   üí° Conseil: Augmenter le seuil de d√©cision (> 0.5)\")\n",
        "    elif fn > fp:\n",
        "        print(f\"   üìä Plus de Faux N√©gatifs: Le mod√®le tend √† sous-classifier comme positif\")\n",
        "        print(f\"   üí° Conseil: Diminuer le seuil de d√©cision (< 0.5)\")\n",
        "    else:\n",
        "        print(f\"   ‚öñÔ∏è Erreurs √©quilibr√©es: Bon seuil de d√©cision\")\n",
        "\n",
        "# G√©n√©ration des visualisations\n",
        "visualiser_matrice_confusion(y_true, y_pred, y_pred_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù Rapport de Classification\n",
        "\n",
        "G√©n√©rons un rapport d√©taill√© avec toutes les m√©triques :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìù Rapport de classification d√©taill√©\n",
        "print(\"üìù RAPPORT DE CLASSIFICATION D√âTAILL√â\\n\")\n",
        "\n",
        "# Rapport scikit-learn\n",
        "class_report = classification_report(y_true, y_pred, \n",
        "                                   target_names=['N√©gatif', 'Positif'],\n",
        "                                   output_dict=True)\n",
        "\n",
        "print(\"üìä M√©triques par classe:\")\n",
        "print(\"‚ïê\" * 70)\n",
        "print(f\"{'Classe':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "print(\"‚ïê\" * 70)\n",
        "\n",
        "for class_name in ['N√©gatif', 'Positif']:\n",
        "    metrics = class_report[class_name]\n",
        "    print(f\"{class_name:<10} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} \"\n",
        "          f\"{metrics['f1-score']:<10.4f} {int(metrics['support']):<10}\")\n",
        "\n",
        "print(\"‚ïê\" * 70)\n",
        "print(f\"{'Macro Avg':<10} {class_report['macro avg']['precision']:<10.4f} \"\n",
        "      f\"{class_report['macro avg']['recall']:<10.4f} \"\n",
        "      f\"{class_report['macro avg']['f1-score']:<10.4f} \"\n",
        "      f\"{int(class_report['macro avg']['support']):<10}\")\n",
        "print(f\"{'Weighted Avg':<10} {class_report['weighted avg']['precision']:<10.4f} \"\n",
        "      f\"{class_report['weighted avg']['recall']:<10.4f} \"\n",
        "      f\"{class_report['weighted avg']['f1-score']:<10.4f} \"\n",
        "      f\"{int(class_report['weighted avg']['support']):<10}\")\n",
        "\n",
        "# Analyse de la qualit√© globale\n",
        "overall_f1 = class_report['weighted avg']['f1-score']\n",
        "print(f\"\\nüèÜ Score F1 Global: {overall_f1:.4f} ({overall_f1*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nüí≠ √âvaluation de la qualit√©:\")\n",
        "if overall_f1 > 0.90:\n",
        "    print(f\"   üåü Qualit√© exceptionnelle - Mod√®le de production\")\n",
        "elif overall_f1 > 0.85:\n",
        "    print(f\"   üéâ Excellente qualit√© - Pr√™t pour d√©ploiement\")\n",
        "elif overall_f1 > 0.80:\n",
        "    print(f\"   ‚úÖ Bonne qualit√© - Utilisable en pratique\")\n",
        "elif overall_f1 > 0.75:\n",
        "    print(f\"   üëç Qualit√© correcte - Am√©liorations possibles\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Qualit√© insuffisante - Optimisation n√©cessaire\")\n",
        "\n",
        "# Comparaison avec un baseline\n",
        "baseline_accuracy = max(np.sum(y_true == 0), np.sum(y_true == 1)) / len(y_true)\n",
        "improvement = test_accuracy - baseline_accuracy\n",
        "\n",
        "print(f\"\\nüìä Comparaison avec baseline:\")\n",
        "print(f\"   üéØ Accuracy mod√®le: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"   üìä Baseline (classe majoritaire): {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
        "print(f\"   üìà Am√©lioration: +{improvement:.4f} (+{improvement*100:.2f} points)\")\n",
        "\n",
        "if improvement > 0.20:\n",
        "    print(f\"   üöÄ Am√©lioration exceptionnelle !\")\n",
        "elif improvement > 0.10:\n",
        "    print(f\"   üéâ Excellente am√©lioration !\")\n",
        "elif improvement > 0.05:\n",
        "    print(f\"   ‚úÖ Bonne am√©lioration\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Am√©lioration modeste\")\n",
        "\n",
        "print(f\"\\nüí° Le mod√®le DistilBERT apporte une valeur significative par rapport √† une approche na√Øve !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Fonction de Pr√©diction Interactive\n",
        "\n",
        "Cr√©ons une fonction pratique pour tester notre mod√®le sur de nouvelles phrases !\n",
        "\n",
        "### üéØ Objectifs de cette Section\n",
        "\n",
        "- **Fonction de pr√©diction** simple et r√©utilisable\n",
        "- **Interface interactive** pour tester des phrases\n",
        "- **Analyse de confiance** des pr√©dictions\n",
        "- **Exemples pratiques** avec diff√©rents types de textes\n",
        "- **Conseils d'interpr√©tation** des r√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÆ Fonction de pr√©diction de sentiment\n",
        "def predire_sentiment(texte, modele=None, tokenizer=tokenizer, max_length=MAX_LENGTH, verbose=True):\n",
        "    \"\"\"\n",
        "    Pr√©dit le sentiment d'un texte donn√©\n",
        "    \n",
        "    Args:\n",
        "        texte (str): Texte √† analyser\n",
        "        modele: Mod√®le √† utiliser (par d√©faut le meilleur mod√®le)\n",
        "        tokenizer: Tokenizer DistilBERT\n",
        "        max_length (int): Longueur maximale de s√©quence\n",
        "        verbose (bool): Affichage d√©taill√©\n",
        "    \n",
        "    Returns:\n",
        "        dict: R√©sultats de pr√©diction\n",
        "    \"\"\"\n",
        "    # Utiliser le meilleur mod√®le par d√©faut\n",
        "    if modele is None:\n",
        "        try:\n",
        "            modele = tf.keras.models.load_model('best_distilbert_model.h5')\n",
        "        except:\n",
        "            modele = best_model  # Fallback sur le mod√®le actuel\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üîÆ Analyse du sentiment pour: \\\"{texte}\\\"\\n\")\n",
        "    \n",
        "    # Tokenisation du texte\n",
        "    tokens = tokenizer(\n",
        "        texte,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üìù Tokenisation:\")\n",
        "        decoded_tokens = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
        "        # Afficher seulement les tokens non-padding\n",
        "        real_tokens = [tok for tok in decoded_tokens if tok != '[PAD]'][:20]  # Limiter l'affichage\n",
        "        print(f\"   üî§ Tokens: {' '.join(real_tokens)}{'...' if len(real_tokens) == 20 else ''}\")\n",
        "        print(f\"   üìè Longueur: {len([tok for tok in decoded_tokens if tok != '[PAD]'])} tokens\")\n",
        "    \n",
        "    # Pr√©diction\n",
        "    prediction = modele([\n",
        "        tokens['input_ids'],\n",
        "        tokens['attention_mask']\n",
        "    ], training=False)\n",
        "    \n",
        "    # Extraction des r√©sultats\n",
        "    probabilite = float(prediction[0][0])\n",
        "    sentiment = \"Positif\" if probabilite > 0.5 else \"N√©gatif\"\n",
        "    confiance = max(probabilite, 1 - probabilite)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nüéØ R√©sultats:\")\n",
        "        print(f\"   üìä Sentiment: {sentiment}\")\n",
        "        print(f\"   üìà Probabilit√©: {probabilite:.4f}\")\n",
        "        print(f\"   üéØ Confiance: {confiance:.4f} ({confiance*100:.1f}%)\")\n",
        "        \n",
        "        # Interpr√©tation de la confiance\n",
        "        if confiance > 0.9:\n",
        "            print(f\"   üí™ Tr√®s haute confiance - Pr√©diction tr√®s fiable\")\n",
        "        elif confiance > 0.8:\n",
        "            print(f\"   ‚úÖ Haute confiance - Pr√©diction fiable\")\n",
        "        elif confiance > 0.7:\n",
        "            print(f\"   üëç Confiance mod√©r√©e - Pr√©diction raisonnable\")\n",
        "        elif confiance > 0.6:\n",
        "            print(f\"   ‚ö†Ô∏è Faible confiance - Pr√©diction incertaine\")\n",
        "        else:\n",
        "            print(f\"   ü§î Tr√®s faible confiance - Texte ambigu\")\n",
        "    \n",
        "    return {\n",
        "        'texte': texte,\n",
        "        'sentiment': sentiment,\n",
        "        'probabilite': probabilite,\n",
        "        'confiance': confiance,\n",
        "        'tokens': len([tok for tok in tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]) if tok != '[PAD]'])\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Fonction de pr√©diction cr√©√©e !\")\n",
        "print(\"üéØ Utilisez: predire_sentiment('Votre texte ici')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß™ Tests sur des Exemples Vari√©s\n",
        "\n",
        "Testons notre mod√®le sur diff√©rents types de textes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ Tests sur des exemples vari√©s\n",
        "exemples_tests = [\n",
        "    # Exemples clairement positifs\n",
        "    \"This movie is absolutely amazing! I loved every minute of it.\",\n",
        "    \"Outstanding performance! One of the best films I've ever seen.\",\n",
        "    \"Brilliant cinematography and excellent acting. Highly recommended!\",\n",
        "    \n",
        "    # Exemples clairement n√©gatifs\n",
        "    \"This movie is terrible. Complete waste of time.\",\n",
        "    \"Awful acting and boring plot. I fell asleep watching it.\",\n",
        "    \"One of the worst movies ever made. Absolutely horrible.\",\n",
        "    \n",
        "    # Exemples neutres/ambigus\n",
        "    \"The movie was okay. Not great, not terrible.\",\n",
        "    \"It has some good moments but also some flaws.\",\n",
        "    \"Average film with decent acting but predictable plot.\",\n",
        "    \n",
        "    # Exemples avec sarcasme/ironie\n",
        "    \"Oh great, another generic superhero movie. Just what we needed.\",\n",
        "    \"Sure, spending 3 hours watching paint dry would be more exciting.\"\n",
        "]\n",
        "\n",
        "print(\"üß™ TESTS SUR EXEMPLES VARI√âS\\n\")\n",
        "print(\"‚ïê\" * 80)\n",
        "\n",
        "resultats_tests = []\n",
        "\n",
        "for i, exemple in enumerate(exemples_tests, 1):\n",
        "    print(f\"\\nüìù Test {i}/{len(exemples_tests)}:\")\n",
        "    print(\"‚îÄ\" * 60)\n",
        "    \n",
        "    resultat = predire_sentiment(exemple, verbose=True)\n",
        "    resultats_tests.append(resultat)\n",
        "    \n",
        "    print(\"‚îÄ\" * 60)\n",
        "\n",
        "print(f\"\\n‚úÖ Tests termin√©s sur {len(exemples_tests)} exemples !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Analyse des R√©sultats de Test\n",
        "\n",
        "Analysons les performances sur nos exemples de test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Analyse des r√©sultats de test\n",
        "def analyser_resultats_tests(resultats):\n",
        "    \"\"\"\n",
        "    Analyse les r√©sultats des tests de pr√©diction\n",
        "    \"\"\"\n",
        "    print(\"üìä ANALYSE DES R√âSULTATS DE TEST\\n\")\n",
        "    \n",
        "    # Cr√©ation d'un DataFrame pour l'analyse\n",
        "    import pandas as pd\n",
        "    \n",
        "    df_results = pd.DataFrame([\n",
        "        {\n",
        "            'Texte': r['texte'][:50] + '...' if len(r['texte']) > 50 else r['texte'],\n",
        "            'Sentiment': r['sentiment'],\n",
        "            'Probabilit√©': f\"{r['probabilite']:.3f}\",\n",
        "            'Confiance': f\"{r['confiance']:.3f}\",\n",
        "            'Tokens': r['tokens']\n",
        "        } for r in resultats\n",
        "    ])\n",
        "    \n",
        "    print(\"üìã Tableau r√©capitulatif:\")\n",
        "    print(df_results.to_string(index=False))\n",
        "    \n",
        "    # Statistiques g√©n√©rales\n",
        "    nb_positifs = sum(1 for r in resultats if r['sentiment'] == 'Positif')\n",
        "    nb_negatifs = len(resultats) - nb_positifs\n",
        "    confiance_moyenne = sum(r['confiance'] for r in resultats) / len(resultats)\n",
        "    \n",
        "    print(f\"\\nüìä Statistiques:\")\n",
        "    print(f\"   üìà Pr√©dictions positives: {nb_positifs}/{len(resultats)} ({nb_positifs/len(resultats)*100:.1f}%)\")\n",
        "    print(f\"   üìâ Pr√©dictions n√©gatives: {nb_negatifs}/{len(resultats)} ({nb_negatifs/len(resultats)*100:.1f}%)\")\n",
        "    print(f\"   üéØ Confiance moyenne: {confiance_moyenne:.3f} ({confiance_moyenne*100:.1f}%)\")\n",
        "    \n",
        "    # Analyse de la confiance\n",
        "    haute_confiance = sum(1 for r in resultats if r['confiance'] > 0.8)\n",
        "    faible_confiance = sum(1 for r in resultats if r['confiance'] < 0.6)\n",
        "    \n",
        "    print(f\"\\nüéØ Analyse de confiance:\")\n",
        "    print(f\"   üí™ Haute confiance (>80%): {haute_confiance}/{len(resultats)} ({haute_confiance/len(resultats)*100:.1f}%)\")\n",
        "    print(f\"   ‚ö†Ô∏è Faible confiance (<60%): {faible_confiance}/{len(resultats)} ({faible_confiance/len(resultats)*100:.1f}%)\")\n",
        "    \n",
        "    # Recommandations\n",
        "    print(f\"\\nüí° Observations:\")\n",
        "    if confiance_moyenne > 0.8:\n",
        "        print(f\"   ‚úÖ Excellent niveau de confiance g√©n√©ral\")\n",
        "    elif confiance_moyenne > 0.7:\n",
        "        print(f\"   üëç Bon niveau de confiance\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Niveau de confiance √† am√©liorer\")\n",
        "    \n",
        "    if faible_confiance > 0:\n",
        "        print(f\"   üîç {faible_confiance} exemple(s) avec faible confiance - textes probablement ambigus\")\n",
        "    \n",
        "    return df_results\n",
        "\n",
        "# Analyse des r√©sultats\n",
        "df_analyse = analyser_resultats_tests(resultats_tests)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Conclusion et R√©capitulatif\n",
        "\n",
        "F√©licitations ! Vous avez compl√©t√© avec succ√®s un projet complet de **fine-tuning DistilBERT** pour l'analyse de sentiment.\n",
        "\n",
        "### üèÜ Ce que Vous Avez Accompli\n",
        "\n",
        "Au cours de ce notebook p√©dagogique, vous avez ma√Ætris√© :\n",
        "\n",
        "#### üîç **Exploration et Pr√©paration des Donn√©es**\n",
        "- Chargement du dataset IMDB (50,000 critiques de films)\n",
        "- Analyse statistique et visualisation de la distribution\n",
        "- Pr√©paration des donn√©es d'entra√Ænement, validation et test\n",
        "- Gestion de datasets d√©s√©quilibr√©s\n",
        "\n",
        "#### üî§ **Tokenisation et Preprocessing NLP**\n",
        "- Utilisation du tokenizer DistilBERT WordPiece\n",
        "- Gestion des tokens sp√©ciaux (`[CLS]`, `[SEP]`, `[PAD]`)\n",
        "- Cr√©ation d'attention masks pour ignorer le padding\n",
        "- Optimisation des s√©quences avec padding et truncation\n",
        "\n",
        "#### üß† **Architecture et Mod√©lisation**\n",
        "- Chargement d'un mod√®le DistilBERT pr√©-entra√Æn√©\n",
        "- Construction d'une t√™te de classification personnalis√©e\n",
        "- Int√©gration du pooling et de la r√©gularisation (dropout)\n",
        "- Compr√©hension du transfer learning en NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Concepts Cl√©s\n",
        "\n",
        "#### üß† **Architecture Transformer**\n",
        "- **Self-attention** : M√©canisme permettant au mod√®le de \"faire attention\" aux mots importants\n",
        "- **Multi-head attention** : Plusieurs t√™tes d'attention pour capturer diff√©rents types de relations\n",
        "- **Positional encodings** : Comment le mod√®le comprend l'ordre des mots\n",
        "- **Layer normalization** : Technique de normalisation pour stabiliser l'entra√Ænement\n",
        "\n",
        "#### üî§ **Tokenisation Avanc√©e**\n",
        "- **WordPiece** : Algorithme de tokenisation sous-mot intelligent\n",
        "- **Vocabulary size** : Impact de la taille du vocabulaire (30,522 tokens pour DistilBERT)\n",
        "- **Out-of-vocabulary handling** : Gestion des mots non vus pendant l'entra√Ænement\n",
        "- **Special tokens** : R√¥le crucial des tokens `[CLS]`, `[SEP]`, `[MASK]`, `[PAD]`\n",
        "\n",
        "#### üéØ **Transfer Learning en NLP**\n",
        "- **Pre-training** : Entra√Ænement sur de vastes corpus de texte non labellis√©\n",
        "- **Fine-tuning** : Adaptation √† une t√¢che sp√©cifique avec peu de donn√©es\n",
        "- **Feature extraction vs Fine-tuning** : Diff√©rentes strat√©gies d'adaptation\n",
        "- **Learning rate scheduling** : Importance d'un LR faible pour pr√©server les connaissances"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
