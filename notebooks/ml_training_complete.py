#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script complet d'entra√Ænement des mod√®les de Machine Learning
Projet PMR - Certification D√©veloppeur IA

Ce script :
1. V√©rifie la pr√©sence des donn√©es pr√©process√©es
2. Ex√©cute le pipeline de donn√©es si n√©cessaire
3. Entra√Æne les mod√®les ML (Random Forest, Logistic Regression)
4. √âvalue les performances
5. Sauvegarde les mod√®les et r√©sultats

Auteur: 0-Octet-1
Date: 8 juillet 2025
"""

import os
import sys
import pandas as pd
import numpy as np
import pickle
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Imports pour ML
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    accuracy_score, f1_score, precision_score, recall_score
)
from sklearn.model_selection import GridSearchCV, cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration des chemins
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_RAW_DIR = os.path.join(PROJECT_ROOT, 'data', 'raw')
DATA_SPLIT_DIR = os.path.join(PROJECT_ROOT, 'data', 'split')
DATA_PREPROCESSED_DIR = os.path.join(PROJECT_ROOT, 'data', 'preprocessed')
MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')
NOTEBOOKS_DIR = os.path.join(PROJECT_ROOT, 'notebooks')

# Cr√©er les dossiers si n√©cessaire
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(DATA_SPLIT_DIR, exist_ok=True)
os.makedirs(DATA_PREPROCESSED_DIR, exist_ok=True)

def print_section(title):
    """Affiche une section avec formatage"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}")

def execute_pipeline_if_needed():
    """
    V√©rifie si les donn√©es pr√©process√©es existent, sinon ex√©cute le pipeline
    """
    print_section("V√âRIFICATION DU PIPELINE DE DONN√âES")
    
    # V√©rifier si les donn√©es pr√©process√©es existent
    preprocessed_files = [
        'X_train_preprocessed.pkl',
        'X_val_preprocessed.pkl', 
        'X_test_preprocessed.pkl',
        'y_train.pkl',
        'y_val.pkl',
        'y_test.pkl'
    ]
    
    all_files_exist = all(
        os.path.exists(os.path.join(DATA_PREPROCESSED_DIR, file)) 
        for file in preprocessed_files
    )
    
    if all_files_exist:
        print("‚úÖ Donn√©es pr√©process√©es trouv√©es")
        return True
    
    print("‚ùå Donn√©es pr√©process√©es manquantes")
    print("üîÑ Ex√©cution du pipeline de donn√©es...")
    
    # Ex√©cuter les scripts du pipeline
    scripts_to_run = [
        'data_generation.py',
        'data_splitting.py', 
        'data_preprocessing.py'
    ]
    
    for script in scripts_to_run:
        script_path = os.path.join(NOTEBOOKS_DIR, script)
        if os.path.exists(script_path):
            print(f"üîÑ Ex√©cution de {script}...")
            try:
                exec(open(script_path).read())
                print(f"‚úÖ {script} termin√©")
            except Exception as e:
                print(f"‚ùå Erreur dans {script}: {e}")
                return False
        else:
            print(f"‚ùå Script {script} non trouv√©")
            return False
    
    print("‚úÖ Pipeline de donn√©es termin√©")
    return True

def load_preprocessed_data():
    """
    Charge les donn√©es pr√©process√©es
    """
    print_section("CHARGEMENT DES DONN√âES PR√âPROCESS√âES")
    
    try:
        # Charger les features
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'X_train_preprocessed.pkl'), 'rb') as f:
            X_train = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'X_val_preprocessed.pkl'), 'rb') as f:
            X_val = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'X_test_preprocessed.pkl'), 'rb') as f:
            X_test = pickle.load(f)
            
        # Charger les targets
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'y_train.pkl'), 'rb') as f:
            y_train = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'y_val.pkl'), 'rb') as f:
            y_val = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'y_test.pkl'), 'rb') as f:
            y_test = pickle.load(f)
            
        print(f"‚úÖ Donn√©es charg√©es:")
        print(f"   - Train: {X_train.shape[0]} √©chantillons, {X_train.shape[1]} features")
        print(f"   - Validation: {X_val.shape[0]} √©chantillons")
        print(f"   - Test: {X_test.shape[0]} √©chantillons")
        
        # Distribution des classes
        print(f"\nüìä Distribution des classes (Train):")
        class_counts = pd.Series(y_train).value_counts().sort_index()
        for class_name, count in class_counts.items():
            percentage = (count / len(y_train)) * 100
            print(f"   - Classe {class_name}: {count} ({percentage:.1f}%)")
            
        return X_train, X_val, X_test, y_train, y_val, y_test
        
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des donn√©es: {e}")
        return None, None, None, None, None, None

def train_random_forest(X_train, y_train, X_val, y_val):
    """
    Entra√Æne un mod√®le Random Forest avec optimisation des hyperparam√®tres
    """
    print_section("ENTRA√éNEMENT RANDOM FOREST")
    
    # D√©finir la grille de param√®tres
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }
    
    # Mod√®le de base
    rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)
    
    # Grid Search avec validation crois√©e
    print("üîç Recherche des meilleurs hyperparam√®tres...")
    grid_search = GridSearchCV(
        rf_base, 
        param_grid, 
        cv=5, 
        scoring='f1_macro',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    # Meilleur mod√®le
    best_rf = grid_search.best_estimator_
    
    print(f"‚úÖ Meilleurs param√®tres: {grid_search.best_params_}")
    print(f"‚úÖ Meilleur score CV: {grid_search.best_score_:.4f}")
    
    # √âvaluation sur validation
    y_val_pred = best_rf.predict(X_val)
    val_f1 = f1_score(y_val, y_val_pred, average='macro')
    val_accuracy = accuracy_score(y_val, y_val_pred)
    
    print(f"üìä Performance sur validation:")
    print(f"   - Accuracy: {val_accuracy:.4f}")
    print(f"   - F1-score macro: {val_f1:.4f}")
    
    return best_rf, grid_search.best_params_, val_f1

def train_logistic_regression(X_train, y_train, X_val, y_val):
    """
    Entra√Æne un mod√®le Logistic Regression avec optimisation des hyperparam√®tres
    """
    print_section("ENTRA√éNEMENT LOGISTIC REGRESSION")
    
    # D√©finir la grille de param√®tres
    param_grid = {
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'penalty': ['l1', 'l2', 'elasticnet'],
        'solver': ['liblinear', 'saga'],
        'max_iter': [1000, 2000]
    }
    
    # Mod√®le de base
    lr_base = LogisticRegression(random_state=42, n_jobs=-1)
    
    # Grid Search avec validation crois√©e
    print("üîç Recherche des meilleurs hyperparam√®tres...")
    grid_search = GridSearchCV(
        lr_base, 
        param_grid, 
        cv=5, 
        scoring='f1_macro',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    # Meilleur mod√®le
    best_lr = grid_search.best_estimator_
    
    print(f"‚úÖ Meilleurs param√®tres: {grid_search.best_params_}")
    print(f"‚úÖ Meilleur score CV: {grid_search.best_score_:.4f}")
    
    # √âvaluation sur validation
    y_val_pred = best_lr.predict(X_val)
    val_f1 = f1_score(y_val, y_val_pred, average='macro')
    val_accuracy = accuracy_score(y_val, y_val_pred)
    
    print(f"üìä Performance sur validation:")
    print(f"   - Accuracy: {val_accuracy:.4f}")
    print(f"   - F1-score macro: {val_f1:.4f}")
    
    return best_lr, grid_search.best_params_, val_f1

def evaluate_model(model, X_test, y_test, model_name):
    """
    √âvalue un mod√®le sur le jeu de test
    """
    print_section(f"√âVALUATION {model_name.upper()}")
    
    # Pr√©dictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)
    
    # M√©triques
    accuracy = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    precision_macro = precision_score(y_test, y_pred, average='macro')
    recall_macro = recall_score(y_test, y_pred, average='macro')
    
    print(f"üìä M√©triques de performance:")
    print(f"   - Accuracy: {accuracy:.4f}")
    print(f"   - F1-score macro: {f1_macro:.4f}")
    print(f"   - F1-score weighted: {f1_weighted:.4f}")
    print(f"   - Precision macro: {precision_macro:.4f}")
    print(f"   - Recall macro: {recall_macro:.4f}")
    
    # Rapport de classification d√©taill√©
    print(f"\nüìã Rapport de classification:")
    print(classification_report(y_test, y_pred))
    
    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    print(f"\nüî¢ Matrice de confusion:")
    print(cm)
    
    # Visualisation de la matrice de confusion
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Matrice de Confusion - {model_name}')
    plt.ylabel('Vraie classe')
    plt.xlabel('Classe pr√©dite')
    plt.tight_layout()
    plt.savefig(os.path.join(MODELS_DIR, f'confusion_matrix_{model_name.lower().replace(" ", "_")}.png'))
    plt.close()
    
    # Importance des features (si disponible)
    if hasattr(model, 'feature_importances_'):
        print(f"\nüéØ Top 10 features importantes:")
        feature_names = [f'feature_{i}' for i in range(len(model.feature_importances_))]
        feature_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(feature_importance.head(10).to_string(index=False))
        
        # Visualisation importance des features
        plt.figure(figsize=(10, 6))
        top_features = feature_importance.head(15)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Importance')
        plt.title(f'Importance des Features - {model_name}')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(os.path.join(MODELS_DIR, f'feature_importance_{model_name.lower().replace(" ", "_")}.png'))
        plt.close()
    
    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'confusion_matrix': cm.tolist(),
        'classification_report': classification_report(y_test, y_pred, output_dict=True)
    }

def save_models_and_results(models_dict, results_dict):
    """
    Sauvegarde les mod√®les et r√©sultats
    """
    print_section("SAUVEGARDE DES MOD√àLES ET R√âSULTATS")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Sauvegarder les mod√®les
    for model_name, model_data in models_dict.items():
        model_filename = f'{model_name.lower().replace(" ", "_")}_{timestamp}.pkl'
        model_path = os.path.join(MODELS_DIR, model_filename)
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"‚úÖ Mod√®le {model_name} sauvegard√©: {model_filename}")
    
    # Sauvegarder les r√©sultats
    results_filename = f'ml_training_results_{timestamp}.json'
    results_path = os.path.join(MODELS_DIR, results_filename)
    
    with open(results_path, 'w') as f:
        json.dump(results_dict, f, indent=2, default=str)
    
    print(f"‚úÖ R√©sultats sauvegard√©s: {results_filename}")
    
    # Cr√©er un r√©sum√© des performances
    summary_filename = f'ml_performance_summary_{timestamp}.txt'
    summary_path = os.path.join(MODELS_DIR, summary_filename)
    
    with open(summary_path, 'w') as f:
        f.write("R√âSUM√â DES PERFORMANCES - MOD√àLES ML\n")
        f.write("="*50 + "\n\n")
        f.write(f"Date d'entra√Ænement: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        for model_name, results in results_dict.items():
            if 'test_results' in results:
                test_results = results['test_results']
                f.write(f"{model_name.upper()}:\n")
                f.write(f"  - Accuracy: {test_results['accuracy']:.4f}\n")
                f.write(f"  - F1-score macro: {test_results['f1_macro']:.4f}\n")
                f.write(f"  - F1-score weighted: {test_results['f1_weighted']:.4f}\n")
                f.write(f"  - Precision macro: {test_results['precision_macro']:.4f}\n")
                f.write(f"  - Recall macro: {test_results['recall_macro']:.4f}\n\n")
    
    print(f"‚úÖ R√©sum√© des performances sauvegard√©: {summary_filename}")

def main():
    """
    Fonction principale
    """
    print_section("SCRIPT D'ENTRA√éNEMENT ML - PROJET PMR")
    print(f"D√©marrage: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. V√©rifier et ex√©cuter le pipeline si n√©cessaire
    if not execute_pipeline_if_needed():
        print("‚ùå √âchec du pipeline de donn√©es")
        return
    
    # 2. Charger les donn√©es pr√©process√©es
    X_train, X_val, X_test, y_train, y_val, y_test = load_preprocessed_data()
    if X_train is None:
        print("‚ùå Impossible de charger les donn√©es")
        return
    
    # 3. Entra√Æner Random Forest
    rf_model, rf_params, rf_val_f1 = train_random_forest(X_train, y_train, X_val, y_val)
    
    # 4. Entra√Æner Logistic Regression
    lr_model, lr_params, lr_val_f1 = train_logistic_regression(X_train, y_train, X_val, y_val)
    
    # 5. √âvaluer sur le jeu de test
    rf_test_results = evaluate_model(rf_model, X_test, y_test, "Random Forest")
    lr_test_results = evaluate_model(lr_model, X_test, y_test, "Logistic Regression")
    
    # 6. Comparer les mod√®les
    print_section("COMPARAISON DES MOD√àLES")
    print(f"Random Forest - F1 macro: {rf_test_results['f1_macro']:.4f}")
    print(f"Logistic Regression - F1 macro: {lr_test_results['f1_macro']:.4f}")
    
    if rf_test_results['f1_macro'] > lr_test_results['f1_macro']:
        print("üèÜ Meilleur mod√®le: Random Forest")
        best_model_name = "Random Forest"
    else:
        print("üèÜ Meilleur mod√®le: Logistic Regression")
        best_model_name = "Logistic Regression"
    
    # V√©rifier l'objectif F1-score ‚â• 75%
    target_f1 = 0.75
    rf_meets_target = rf_test_results['f1_macro'] >= target_f1
    lr_meets_target = lr_test_results['f1_macro'] >= target_f1
    
    print(f"\nüéØ Objectif F1-score ‚â• {target_f1:.0%}:")
    print(f"   - Random Forest: {'‚úÖ' if rf_meets_target else '‚ùå'} ({rf_test_results['f1_macro']:.1%})")
    print(f"   - Logistic Regression: {'‚úÖ' if lr_meets_target else '‚ùå'} ({lr_test_results['f1_macro']:.1%})")
    
    # 7. Sauvegarder tout
    models_dict = {
        'Random Forest': {
            'model': rf_model,
            'best_params': rf_params,
            'validation_f1': rf_val_f1
        },
        'Logistic Regression': {
            'model': lr_model,
            'best_params': lr_params,
            'validation_f1': lr_val_f1
        }
    }
    
    results_dict = {
        'Random Forest': {
            'best_params': rf_params,
            'validation_f1': rf_val_f1,
            'test_results': rf_test_results
        },
        'Logistic Regression': {
            'best_params': lr_params,
            'validation_f1': lr_val_f1,
            'test_results': lr_test_results
        },
        'comparison': {
            'best_model': best_model_name,
            'target_f1_achieved': rf_meets_target or lr_meets_target
        }
    }
    
    save_models_and_results(models_dict, results_dict)
    
    print_section("ENTRA√éNEMENT TERMIN√â")
    print(f"Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("‚úÖ Tous les mod√®les ML ont √©t√© entra√Æn√©s et sauvegard√©s")
    print(f"üèÜ Meilleur mod√®le: {best_model_name}")
    print(f"üìÅ R√©sultats disponibles dans: {MODELS_DIR}")

if __name__ == "__main__":
    main()
