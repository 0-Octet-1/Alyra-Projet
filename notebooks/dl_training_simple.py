#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script simplifi√© d'entra√Ænement Deep Learning avec MLPClassifier
Projet PMR - Version robuste et stable

Auteur: 0-Octet-1
Date: 8 juillet 2025
"""

import os
import pandas as pd
import numpy as np
import pickle
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    accuracy_score, f1_score, precision_score, recall_score
)

# Configuration des chemins
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_PREPROCESSED_DIR = os.path.join(PROJECT_ROOT, 'data', 'preprocessed')
MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')

os.makedirs(MODELS_DIR, exist_ok=True)

def print_section(title):
    """Affiche une section avec formatage"""
    print(f"\n{'='*50}")
    print(f"  {title}")
    print(f"{'='*50}")

def load_data():
    """Charge les donn√©es pr√©process√©es"""
    print_section("CHARGEMENT DES DONN√âES")
    
    try:
        # Charger les features
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'X_train.pkl'), 'rb') as f:
            X_train = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'X_val.pkl'), 'rb') as f:
            X_val = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'X_test.pkl'), 'rb') as f:
            X_test = pickle.load(f)
            
        # Charger les targets
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'y_train.pkl'), 'rb') as f:
            y_train = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'y_val.pkl'), 'rb') as f:
            y_val = pickle.load(f)
        with open(os.path.join(DATA_PREPROCESSED_DIR, 'y_test.pkl'), 'rb') as f:
            y_test = pickle.load(f)
        
        # Convertir en arrays numpy si n√©cessaire
        if hasattr(X_train, 'values'):
            X_train = X_train.values
            X_val = X_val.values
            X_test = X_test.values
        
        # Combiner train et validation
        X_train_full = np.vstack([X_train, X_val])
        y_train_full = np.hstack([y_train, y_val])
        
        print(f"‚úÖ Donn√©es charg√©es:")
        print(f"   - Train: {X_train_full.shape[0]} √©chantillons, {X_train_full.shape[1]} features")
        print(f"   - Test: {X_test.shape[0]} √©chantillons")
        print(f"   - Classes: {np.unique(y_train_full)}")
        
        return X_train_full, X_test, y_train_full, y_test
        
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        return None, None, None, None

def train_simple_mlp(X_train, y_train, X_test, y_test):
    """Entra√Æne un MLP simple et robuste"""
    print_section("ENTRA√éNEMENT MLP SIMPLE")
    
    try:
        # Mod√®le MLP simple
        mlp = MLPClassifier(
            hidden_layer_sizes=(100, 50),
            activation='relu',
            solver='adam',
            alpha=0.001,
            learning_rate_init=0.001,
            max_iter=300,
            early_stopping=True,
            validation_fraction=0.2,
            n_iter_no_change=10,
            random_state=42,
            verbose=False
        )
        
        print("üîÑ Entra√Ænement en cours...")
        mlp.fit(X_train, y_train)
        
        print(f"‚úÖ Entra√Ænement termin√© !")
        print(f"   It√©rations: {mlp.n_iter_}")
        
        # Pr√©dictions
        y_pred = mlp.predict(X_test)
        
        # M√©triques
        accuracy = accuracy_score(y_test, y_pred)
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        precision_macro = precision_score(y_test, y_pred, average='macro')
        recall_macro = recall_score(y_test, y_pred, average='macro')
        
        print(f"\nüìä R√âSULTATS MLP:")
        print(f"   - Accuracy: {accuracy:.4f}")
        print(f"   - F1-score macro: {f1_macro:.4f}")
        print(f"   - F1-score weighted: {f1_weighted:.4f}")
        print(f"   - Precision macro: {precision_macro:.4f}")
        print(f"   - Recall macro: {recall_macro:.4f}")
        
        # Matrice de confusion
        cm = confusion_matrix(y_test, y_pred)
        print(f"\nüî¢ Matrice de confusion:")
        print(cm)
        
        return {
            'model': mlp,
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'confusion_matrix': cm.tolist(),
            'n_iterations': mlp.n_iter_
        }
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'entra√Ænement: {e}")
        return None

def train_deep_mlp(X_train, y_train, X_test, y_test):
    """Entra√Æne un MLP plus profond"""
    print_section("ENTRA√éNEMENT MLP PROFOND")
    
    try:
        # Mod√®le MLP profond
        mlp = MLPClassifier(
            hidden_layer_sizes=(128, 64, 32),
            activation='relu',
            solver='adam',
            alpha=0.001,
            learning_rate_init=0.001,
            max_iter=300,
            early_stopping=True,
            validation_fraction=0.2,
            n_iter_no_change=10,
            random_state=42,
            verbose=False
        )
        
        print("üîÑ Entra√Ænement en cours...")
        mlp.fit(X_train, y_train)
        
        print(f"‚úÖ Entra√Ænement termin√© !")
        print(f"   It√©rations: {mlp.n_iter_}")
        
        # Pr√©dictions
        y_pred = mlp.predict(X_test)
        
        # M√©triques
        accuracy = accuracy_score(y_test, y_pred)
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        precision_macro = precision_score(y_test, y_pred, average='macro')
        recall_macro = recall_score(y_test, y_pred, average='macro')
        
        print(f"\nüìä R√âSULTATS MLP PROFOND:")
        print(f"   - Accuracy: {accuracy:.4f}")
        print(f"   - F1-score macro: {f1_macro:.4f}")
        print(f"   - F1-score weighted: {f1_weighted:.4f}")
        print(f"   - Precision macro: {precision_macro:.4f}")
        print(f"   - Recall macro: {recall_macro:.4f}")
        
        # Matrice de confusion
        cm = confusion_matrix(y_test, y_pred)
        print(f"\nüî¢ Matrice de confusion:")
        print(cm)
        
        return {
            'model': mlp,
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'confusion_matrix': cm.tolist(),
            'n_iterations': mlp.n_iter_
        }
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'entra√Ænement: {e}")
        return None

def compare_with_ml(mlp_results):
    """Compare avec les mod√®les ML existants"""
    print_section("COMPARAISON ML vs MLP")
    
    try:
        # Charger les r√©sultats ML
        ml_files = [f for f in os.listdir(MODELS_DIR) if f.startswith('ml_results_') and f.endswith('.json')]
        
        if ml_files:
            latest_ml_file = sorted(ml_files)[-1]
            ml_path = os.path.join(MODELS_DIR, latest_ml_file)
            
            with open(ml_path, 'r') as f:
                ml_results = json.load(f)
            
            print("üìä COMPARAISON DES PERFORMANCES:")
            print(f"{'Mod√®le':<20} {'Accuracy':<10} {'F1-macro':<10}")
            print("-" * 40)
            
            # Mod√®les ML
            if 'random_forest' in ml_results:
                rf = ml_results['random_forest']
                print(f"{'Random Forest':<20} {rf['accuracy']:<10.4f} {rf['f1_macro']:<10.4f}")
            
            if 'logistic_regression' in ml_results:
                lr = ml_results['logistic_regression']
                print(f"{'Logistic Regression':<20} {lr['accuracy']:<10.4f} {lr['f1_macro']:<10.4f}")
            
            # Mod√®les MLP
            for name, results in mlp_results.items():
                if results and 'f1_macro' in results:
                    print(f"{name:<20} {results['accuracy']:<10.4f} {results['f1_macro']:<10.4f}")
            
            # Meilleur mod√®le
            all_models = {}
            if 'random_forest' in ml_results:
                all_models['Random Forest'] = ml_results['random_forest']['f1_macro']
            if 'logistic_regression' in ml_results:
                all_models['Logistic Regression'] = ml_results['logistic_regression']['f1_macro']
            
            for name, results in mlp_results.items():
                if results and 'f1_macro' in results:
                    all_models[name] = results['f1_macro']
            
            if all_models:
                best_model = max(all_models, key=all_models.get)
                best_f1 = all_models[best_model]
                
                print(f"\nüèÜ MEILLEUR MOD√àLE: {best_model}")
                print(f"üìä F1-score: {best_f1:.4f}")
                
                # Objectif
                target_f1 = 0.75
                status = "‚úÖ" if best_f1 >= target_f1 else "‚ùå"
                print(f"\nüéØ Objectif F1 ‚â• 75%: {status} ({best_f1:.1%})")
                
                return best_model, best_f1
        
        return None, None
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la comparaison: {e}")
        return None, None

def save_results(mlp_results):
    """Sauvegarde les r√©sultats"""
    print_section("SAUVEGARDE")
    
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarder les mod√®les
        for name, results in mlp_results.items():
            if results and 'model' in results:
                model_filename = f'mlp_{name.lower().replace(" ", "_")}_{timestamp}.pkl'
                model_path = os.path.join(MODELS_DIR, model_filename)
                
                with open(model_path, 'wb') as f:
                    pickle.dump(results['model'], f)
                
                print(f"‚úÖ {name} sauvegard√©: {model_filename}")
        
        # Sauvegarder les r√©sultats
        results_to_save = {}
        for name, results in mlp_results.items():
            if results:
                results_copy = results.copy()
                if 'model' in results_copy:
                    del results_copy['model']
                results_to_save[name] = results_copy
        
        results_data = {
            'timestamp': timestamp,
            'models': results_to_save,
            'framework': 'scikit-learn MLPClassifier'
        }
        
        results_filename = f'mlp_results_{timestamp}.json'
        results_path = os.path.join(MODELS_DIR, results_filename)
        
        with open(results_path, 'w') as f:
            json.dump(results_data, f, indent=2, default=str)
        
        print(f"‚úÖ R√©sultats sauvegard√©s: {results_filename}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde: {e}")

def main():
    """Fonction principale"""
    print_section("DEEP LEARNING SIMPLIFI√â - PROJET PMR")
    print(f"D√©marrage: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. Charger les donn√©es
    X_train, X_test, y_train, y_test = load_data()
    
    if X_train is None:
        print("‚ùå Impossible de charger les donn√©es. Arr√™t du script.")
        return
    
    # 2. Entra√Æner les mod√®les MLP
    mlp_results = {}
    
    # MLP Simple
    simple_results = train_simple_mlp(X_train, y_train, X_test, y_test)
    if simple_results:
        mlp_results['MLP Simple'] = simple_results
    
    # MLP Profond
    deep_results = train_deep_mlp(X_train, y_train, X_test, y_test)
    if deep_results:
        mlp_results['MLP Profond'] = deep_results
    
    # 3. Comparer les r√©sultats
    if mlp_results:
        best_mlp = max(mlp_results, key=lambda x: mlp_results[x]['f1_macro'])
        print(f"\nüèÜ Meilleur MLP: {best_mlp}")
        print(f"üìä F1-score: {mlp_results[best_mlp]['f1_macro']:.4f}")
        
        # 4. Comparer avec ML
        best_global, best_f1 = compare_with_ml(mlp_results)
        
        # 5. Sauvegarder
        save_results(mlp_results)
        
        print_section("TERMIN√â")
        print(f"‚úÖ Entra√Ænement Deep Learning termin√© avec succ√®s !")
        if best_global:
            print(f"üåü Meilleur mod√®le global: {best_global} (F1: {best_f1:.4f})")
    else:
        print("‚ùå Aucun mod√®le MLP n'a pu √™tre entra√Æn√©.")

if __name__ == "__main__":
    main()
