"""
ğŸ—ï¸ PROJET PMR - PRÃ‰PARATION DES DONNÃ‰ES
ğŸ“š Certification DÃ©veloppeur IA - Alyra
ğŸ¯ Bloc 03 & 05 - Machine Learning & Deep Learning

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“– OBJECTIF PÃ‰DAGOGIQUE :
Ce script gÃ©nÃ¨re un dataset synthÃ©tique pour prÃ©dire l'accessibilitÃ© PMR de points d'intÃ©rÃªt urbains.
Chaque Ã©tape est expliquÃ©e pour comprendre les choix techniques lors de la soutenance.

ğŸ¯ CLASSES CIBLES :
- "Facilement Accessible" 
- "ModÃ©rÃ©ment Accessible"
- "Difficilement Accessible"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# Configuration pour les graphiques
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("ğŸš€ DÃ‰BUT - PrÃ©paration des DonnÃ©es PMR")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š Ã‰TAPE 1 : GÃ‰NÃ‰RATION DU DATASET SYNTHÃ‰TIQUE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ“Š Ã‰TAPE 1 : GÃ©nÃ©ration du Dataset SynthÃ©tique")
print("-" * 50)

def generer_dataset_pmr(n_samples=4000):
    """
    ğŸ¯ FONCTION PÃ‰DAGOGIQUE : GÃ©nÃ©ration du dataset synthÃ©tique PMR
    
    POURQUOI SYNTHÃ‰TIQUE ?
    âœ… ContrÃ´le total des variables et de leur distribution
    âœ… Pas de problÃ¨mes de confidentialitÃ© (donnÃ©es RGPD)
    âœ… DiversitÃ© garantie des cas d'usage
    âœ… Labels cohÃ©rents avec la logique mÃ©tier PMR
    
    PARAMÃˆTRES :
    - n_samples : Nombre d'exemples Ã  gÃ©nÃ©rer (recommandÃ© : 3000-6000)
    """
    
    print(f"   ğŸ”„ GÃ©nÃ©ration de {n_samples} exemples...")
    
    # Initialisation du gÃ©nÃ©rateur alÃ©atoire pour la reproductibilitÃ©
    np.random.seed(42)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ“ VARIABLES DESCRIPTIVES (CaractÃ©ristiques physiques)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # Largeur du trottoir (cm) - Distribution normale avec queue Ã  droite
    largeur_trottoir_cm = np.random.gamma(3, 40, n_samples)  # Moyenne ~120cm, min ~50cm
    largeur_trottoir_cm = np.clip(largeur_trottoir_cm, 50, 300)
    
    # Pente d'accÃ¨s (degrÃ©s) - Majoritairement faible, quelques cas difficiles
    pente_acces_degres = np.random.exponential(2.5, n_samples)  # Moyenne ~2.5Â°
    pente_acces_degres = np.clip(pente_acces_degres, 0, 25)
    
    # Distance au transport (mÃ¨tres) - Distribution uniforme
    distance_transport_m = np.random.randint(0, 1001, n_samples)
    
    # Nombre d'obstacles fixes - Distribution de Poisson
    nombre_obstacles_fixes = np.random.poisson(1.2, n_samples)
    nombre_obstacles_fixes = np.clip(nombre_obstacles_fixes, 0, 8)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ—ï¸ VARIABLES INFRASTRUCTURELLES (Ã‰quipements et Ã©tat)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # PrÃ©sence d'ascenseur (boolÃ©enne)
    presence_ascenseur = np.random.choice([True, False], n_samples, p=[0.3, 0.7])
    
    # Ã‰tat de l'ascenseur (conditionnÃ© par sa prÃ©sence)
    etat_ascenseur = []
    for has_elevator in presence_ascenseur:
        if has_elevator:
            etat_ascenseur.append(np.random.choice([
                "fonctionnel", "en_panne_occasionnelle", "hors_service"
            ], p=[0.7, 0.2, 0.1]))
        else:
            etat_ascenseur.append("non_applicable")
    
    # Type de revÃªtement du sol
    type_revetement_sol = np.random.choice([
        "lisse", "pavÃ©", "gravier", "terre"
    ], n_samples, p=[0.5, 0.3, 0.15, 0.05])
    
    # Ã‚ge de l'infrastructure (annÃ©es)
    age_infrastructure_an = np.random.gamma(2, 15, n_samples)  # Moyenne ~30 ans
    age_infrastructure_an = np.clip(age_infrastructure_an, 0, 100).astype(int)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸŒ VARIABLES CONTEXTUELLES (Environnement et usage)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # Type de lieu
    type_lieu = np.random.choice([
        "commerce", "musÃ©e", "gare", "restaurant", 
        "hÃ´pital", "hÃ´tel", "mairie", "Ã©cole"
    ], n_samples, p=[0.25, 0.1, 0.15, 0.2, 0.1, 0.05, 0.05, 0.1])
    
    # Type de transport en proximitÃ©
    type_transport_proximite = np.random.choice([
        "bus", "mÃ©tro", "tram", "rer", "aucun"
    ], n_samples, p=[0.3, 0.2, 0.15, 0.1, 0.25])
    
    # Zone gÃ©ographique
    zone_geographique = np.random.choice([
        "centre_historique", "rÃ©sidentiel", "affaires", "pÃ©riphÃ©rie"
    ], n_samples, p=[0.2, 0.4, 0.25, 0.15])
    
    # Affluence en heure de pointe
    affluence_heure_pointe = np.random.choice([
        "faible", "moyenne", "forte"
    ], n_samples, p=[0.3, 0.5, 0.2])
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ¯ GÃ‰NÃ‰RATION DES LABELS (Logique mÃ©tier PMR)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("   ğŸ§  Application de la logique mÃ©tier pour les labels...")
    
    labels = []
    for i in range(n_samples):
        score_accessibilite = 0
        
        # ğŸ“ Facteurs descriptifs (40% du score)
        if largeur_trottoir_cm[i] >= 140:
            score_accessibilite += 15
        elif largeur_trottoir_cm[i] >= 90:
            score_accessibilite += 10
        else:
            score_accessibilite += 5
            
        if pente_acces_degres[i] <= 2:
            score_accessibilite += 15
        elif pente_acces_degres[i] <= 5:
            score_accessibilite += 10
        else:
            score_accessibilite += 5
            
        if distance_transport_m[i] <= 200:
            score_accessibilite += 5
        elif distance_transport_m[i] <= 500:
            score_accessibilite += 3
            
        score_accessibilite -= min(nombre_obstacles_fixes[i] * 3, 15)
        
        # ğŸ—ï¸ Facteurs infrastructurels (35% du score)
        if presence_ascenseur[i] and etat_ascenseur[i] == "fonctionnel":
            score_accessibilite += 20
        elif presence_ascenseur[i] and etat_ascenseur[i] == "en_panne_occasionnelle":
            score_accessibilite += 10
        elif presence_ascenseur[i] and etat_ascenseur[i] == "hors_service":
            score_accessibilite += 5
            
        if type_revetement_sol[i] == "lisse":
            score_accessibilite += 10
        elif type_revetement_sol[i] == "pavÃ©":
            score_accessibilite += 7
        elif type_revetement_sol[i] == "gravier":
            score_accessibilite += 3
            
        if age_infrastructure_an[i] <= 10:
            score_accessibilite += 5
        elif age_infrastructure_an[i] <= 30:
            score_accessibilite += 3
            
        # ğŸŒ Facteurs contextuels (25% du score)
        if type_lieu[i] in ["hÃ´pital", "mairie"]:
            score_accessibilite += 10  # Obligation lÃ©gale renforcÃ©e
        elif type_lieu[i] in ["gare", "Ã©cole"]:
            score_accessibilite += 7
        elif type_lieu[i] in ["musÃ©e", "hÃ´tel"]:
            score_accessibilite += 5
            
        if zone_geographique[i] == "centre_historique":
            score_accessibilite -= 5  # Contraintes patrimoine
        elif zone_geographique[i] == "affaires":
            score_accessibilite += 3  # Standards modernes
            
        # ğŸ¯ Attribution du label final
        if score_accessibilite >= 70:
            labels.append("Facilement Accessible")
        elif score_accessibilite >= 45:
            labels.append("ModÃ©rÃ©ment Accessible")
        else:
            labels.append("Difficilement Accessible")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ“‹ ASSEMBLAGE DU DATAFRAME FINAL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    dataset = pd.DataFrame({
        # Variables descriptives
        'largeur_trottoir_cm': largeur_trottoir_cm,
        'pente_acces_degres': pente_acces_degres,
        'distance_transport_m': distance_transport_m,
        'nombre_obstacles_fixes': nombre_obstacles_fixes,
        
        # Variables infrastructurelles
        'presence_ascenseur': presence_ascenseur,
        'etat_ascenseur': etat_ascenseur,
        'type_revetement_sol': type_revetement_sol,
        'age_infrastructure_an': age_infrastructure_an,
        
        # Variables contextuelles
        'type_lieu': type_lieu,
        'type_transport_proximite': type_transport_proximite,
        'zone_geographique': zone_geographique,
        'affluence_heure_pointe': affluence_heure_pointe,
        
        # Variable cible
        'difficulte_accessibilite': labels
    })
    
    print(f"   âœ… Dataset gÃ©nÃ©rÃ© : {len(dataset)} exemples")
    return dataset

# GÃ©nÃ©ration du dataset principal
df_pmr = generer_dataset_pmr(4000)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š Ã‰TAPE 2 : ANALYSE EXPLORATOIRE RAPIDE (EDA)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ“Š Ã‰TAPE 2 : Analyse Exploratoire des DonnÃ©es (EDA)")
print("-" * 50)

# Informations gÃ©nÃ©rales sur le dataset
print("\nğŸ“‹ INFORMATIONS GÃ‰NÃ‰RALES :")
print(f"   â€¢ Nombre d'exemples : {len(df_pmr)}")
print(f"   â€¢ Nombre de features : {len(df_pmr.columns)-1}")
print(f"   â€¢ MÃ©moire utilisÃ©e : {df_pmr.memory_usage(deep=True).sum() / 1024:.1f} KB")

# Distribution des classes cibles
print("\nğŸ¯ DISTRIBUTION DES CLASSES CIBLES :")
class_counts = df_pmr['difficulte_accessibilite'].value_counts()
class_props = df_pmr['difficulte_accessibilite'].value_counts(normalize=True)

for classe, count in class_counts.items():
    prop = class_props[classe]
    print(f"   â€¢ {classe:<25} : {count:>4} ({prop:.1%})")

# VÃ©rification du dÃ©sÃ©quilibre des classes
print(f"\nâš–ï¸  DÃ‰SÃ‰QUILIBRE DES CLASSES :")
ratio_min_max = class_counts.min() / class_counts.max()
print(f"   â€¢ Ratio min/max : {ratio_min_max:.2f}")
if ratio_min_max < 0.7:
    print("   â€¢ âš ï¸  DÃ©sÃ©quilibre dÃ©tectÃ© - Ã€ surveiller lors de l'Ã©valuation")
else:
    print("   â€¢ âœ… Classes relativement Ã©quilibrÃ©es")

# AperÃ§u des premiÃ¨res lignes
print("\nğŸ‘€ APERÃ‡U DES DONNÃ‰ES :")
print(df_pmr.head(3).to_string())

# VÃ©rification des valeurs manquantes
print("\nğŸ” VÃ‰RIFICATION DES VALEURS MANQUANTES :")
missing_counts = df_pmr.isnull().sum()
if missing_counts.sum() == 0:
    print("   â€¢ âœ… Aucune valeur manquante dÃ©tectÃ©e")
else:
    print("   â€¢ âš ï¸  Valeurs manquantes trouvÃ©es :")
    for col, count in missing_counts[missing_counts > 0].items():
        print(f"     - {col}: {count}")

print("\n" + "="*60)
print("âœ… DATASET PMR GÃ‰NÃ‰RÃ‰ AVEC SUCCÃˆS !")
print("ğŸ“Š PrÃªt pour le preprocessing et la modÃ©lisation")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§¹ Ã‰TAPE 3 : PREPROCESSING DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ§¹ Ã‰TAPE 3 : Preprocessing et PrÃ©paration pour ModÃ©lisation")
print("-" * 50)

def preprocess_dataset(df, save_path=None, save_visualizations=True):
    """
    ğŸ¯ FONCTION PÃ‰DAGOGIQUE : Preprocessing du dataset PMR
    
    âœ… Une Ã©tape critique pour garantir des modÃ¨les performants
    âœ… Traitement adaptÃ© selon type de variable
    âœ… Standardisation pour variables numÃ©riques
    âœ… Encodage one-hot pour variables catÃ©gorielles
    
    PARAMÃˆTRES :
    - df : DataFrame Ã  prÃ©traiter
    - save_path : Chemin pour sauvegarder les donnÃ©es (optionnel)
    """
    print("\n   ğŸ“‹ SÃ©paration features / target...")
    
    # SÃ©paration features / target
    X = df.drop('difficulte_accessibilite', axis=1)
    y = df['difficulte_accessibilite']
    
    print("   ğŸ” Identification des types de variables...")
    
    # Identification des types de variables
    cat_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()
    num_cols = X.select_dtypes(include=['number']).columns.tolist()
    
    print(f"     â€¢ Variables catÃ©gorielles ({len(cat_cols)}) : {', '.join(cat_cols)}")
    print(f"     â€¢ Variables numÃ©riques ({len(num_cols)}) : {', '.join(num_cols)}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ› ï¸ CRÃ‰ATION DU PIPELINE DE PREPROCESSING
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n   ğŸ› ï¸ Construction du pipeline de preprocessing...")
    
    # Preprocessing pour variables numÃ©riques
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),  # Gestion valeurs manquantes
        ('scaler', StandardScaler())                    # Standardisation
    ])
    
    # Preprocessing pour variables catÃ©gorielles
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),  # Gestion valeurs manquantes
        ('onehot', OneHotEncoder(drop='first', sparse_output=False))  # Encodage one-hot
    ])
    
    # Assemblage du preprocessor complet
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, num_cols),
            ('cat', categorical_transformer, cat_cols)
        ],
        remainder='drop'  # Variables non spÃ©cifiÃ©es sont ignorÃ©es
    )
    
    print("   ğŸ”„ Application du preprocessing...")
    
    # Application du preprocessing
    X_processed = preprocessor.fit_transform(X)
    
    # RÃ©cupÃ©ration des noms des colonnes aprÃ¨s one-hot encoding
    onehot_cols = []
    for cat_col in cat_cols:
        if cat_col == 'presence_ascenseur':
            onehot_cols.append(f"{cat_col}_True")
        else:
            encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']
            categories = encoder.categories_[cat_cols.index(cat_col)]
            # La premiÃ¨re catÃ©gorie est droppÃ©e avec drop='first'
            onehot_cols.extend([f"{cat_col}_{cat}" for cat in categories[1:]])
    
    processed_cols = num_cols + onehot_cols
    
    # CrÃ©ation du DataFrame prÃ©traitÃ©
    X_processed_df = pd.DataFrame(
        X_processed, 
        columns=processed_cols,
        index=X.index
    )
    
    print(f"   âœ… Preprocessing terminÃ© : {X_processed.shape[1]} features aprÃ¨s encodage")
    print(f"      (vs {X.shape[1]} features avant)")
    
    # Sauvegarde des donnÃ©es
    if save_path:
        import pickle
        import os
        
        # CrÃ©ation du dossier s'il n'existe pas
        os.makedirs(save_path, exist_ok=True)
        print(f"   ğŸ“ Dossier crÃ©Ã©/vÃ©rifiÃ© : {save_path}")
        
        with open(f"{save_path}/X_train.pkl", "wb") as f:
            pickle.dump(X_processed_df, f)
        with open(f"{save_path}/y_train.pkl", "wb") as f:
            pickle.dump(y, f)
        
        print(f"   ğŸ’¾ DonnÃ©es prÃ©traitÃ©es sauvegardÃ©es dans {save_path}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ“ˆ VISUALISATION DES DONNÃ‰ES PRÃ‰TRAITÃ‰ES
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    print("\n   ğŸ“Š CrÃ©ation de visualisations...")
    
    # Visualisation des distributions standardisÃ©es (si demandÃ©)
    if save_visualizations:
        try:
            num_features = list(X_processed_df.columns)
            
            if len(num_features) > 0:  # S'il y a des features numÃ©riques Ã  visualiser
                plt.figure(figsize=(15, 10))
                for i, col in enumerate(num_features[:min(len(num_features), 15)]):
                    plt.subplot(3, 5, i+1)
                    sns.histplot(X_processed_df[col], kde=True)
                    plt.title(f'{col}\nMoy: {X_processed_df[col].mean():.2f}, Std: {X_processed_df[col].std():.2f}')
                    plt.tight_layout()
                    
                if save_path:
                    plt.savefig(f"{save_path}/distributions_standardisees.png")
                plt.show()
                
                # Matrice de corrÃ©lation (si plus de 2 features)
                if len(num_features) > 2:
                    plt.figure(figsize=(12, 10))
                    corr_matrix = X_processed_df.corr()
                    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
                    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', 
                                cmap='coolwarm', square=True, linewidths=0.5)
                    plt.title('Matrice de corrÃ©lation des features prÃ©traitÃ©es')
                    plt.tight_layout()
                    
                    if save_path:
                        plt.savefig(f"{save_path}/correlation_matrix.png")
                    plt.show()
        except Exception as e:
            print(f"âš ï¸ Avertissement : Erreur lors de la gÃ©nÃ©ration des visualisations : {e}")
            print("âš ï¸ Continuation du script sans les visualisations...")
    
    return X_processed_df, y, preprocessor

# CrÃ©ation du dossier data/preprocessed si nÃ©cessaire
import os
os.makedirs("e:/alyra/Projet/PMR/data/preprocessed", exist_ok=True)

# Application du preprocessing
X_processed, y, preprocessor = preprocess_dataset(df_pmr, save_path="e:/alyra/Projet/PMR/data/preprocessed", save_visualizations=False)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”„ Ã‰TAPE 4 : SPLIT DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ”„ Ã‰TAPE 4 : Split Train/Validation/Test")
print("-" * 50)

def split_dataset(X, y, val_size=0.15, test_size=0.15):
    """
    ğŸ¯ FONCTION PÃ‰DAGOGIQUE : Split stratifiÃ© du dataset PMR
    
    âœ… Split stratifiÃ© pour conserver la distribution des classes
    âœ… Division en 3 sets : Train (70%), Validation (15%), Test (15%)
    
    PARAMÃˆTRES :
    - X : Features prÃ©traitÃ©es
    - y : Target (classes d'accessibilitÃ©)
    - val_size : Taille relative du set de validation
    - test_size : Taille relative du set de test
    """
    print("\n   ğŸ”ª Split des donnÃ©es en Train/Validation/Test...")
    
    # Premier split pour sÃ©parer train et test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )
    
    # Second split pour sÃ©parer train et validation
    # Recalcul du ratio pour validation
    val_ratio = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_ratio, random_state=42, stratify=y_temp
    )
    
    print(f"   âœ… Split terminÃ©:")
    print(f"     â€¢ Train: {X_train.shape[0]} exemples ({X_train.shape[0]/len(X):.1%})")
    print(f"     â€¢ Validation: {X_val.shape[0]} exemples ({X_val.shape[0]/len(X):.1%})")
    print(f"     â€¢ Test: {X_test.shape[0]} exemples ({X_test.shape[0]/len(X):.1%})")
    
    # VÃ©rification de la stratification
    print("\n   ğŸ” VÃ©rification de la stratification des classes:")
    for name, y_set in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:
        class_props = pd.Series(y_set).value_counts(normalize=True)
        print(f"     â€¢ {name}: {', '.join([f'{c}: {p:.1%}' for c, p in class_props.items()])}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# Split des donnÃ©es
X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X_processed, y)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¾ Ã‰TAPE 5 : SAUVEGARDE DES DONNÃ‰ES PRÃ‰TRAITÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ’¾ Ã‰TAPE 5 : Sauvegarde des donnÃ©es prÃ©traitÃ©es")
print("-" * 50)

def save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test, preprocessor, folder="e:/alyra/Projet/PMR/data/preprocessed"):
    """
    ğŸ¯ FONCTION PÃ‰DAGOGIQUE : Sauvegarde des donnÃ©es pour modÃ©lisation
    
    âœ… Format pickle pour assurer la prÃ©servation des types
    âœ… Sauvegarde du preprocessor pour application future
    
    PARAMÃˆTRES :
    - X_train, X_val, X_test : Features divisÃ©es
    - y_train, y_val, y_test : Target divisÃ©es
    - preprocessor : Pipeline de preprocessing
    - folder : Dossier de destination
    """
    print("\n   ğŸ“ CrÃ©ation du dossier de sauvegarde...")
    
    # CrÃ©ation du dossier s'il n'existe pas
    import os
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"     â€¢ Dossier crÃ©Ã© : {folder}")
    
    print("   ğŸ’¾ Sauvegarde des donnÃ©es...")
    
    # Sauvegarde des datasets
    import pickle
    
    with open(f"{folder}/X_train.pkl", "wb") as f:
        pickle.dump(X_train, f)
    
    with open(f"{folder}/X_val.pkl", "wb") as f:
        pickle.dump(X_val, f)
    
    with open(f"{folder}/X_test.pkl", "wb") as f:
        pickle.dump(X_test, f)
    
    with open(f"{folder}/y_train.pkl", "wb") as f:
        pickle.dump(y_train, f)
    
    with open(f"{folder}/y_val.pkl", "wb") as f:
        pickle.dump(y_val, f)
    
    with open(f"{folder}/y_test.pkl", "wb") as f:
        pickle.dump(y_test, f)
    
    with open(f"{folder}/preprocessor.pkl", "wb") as f:
        pickle.dump(preprocessor, f)
    
    # Sauvegarde Ã©galement en CSV pour inspection visuelle
    pd.DataFrame(X_train).to_csv(f"{folder}/X_train.csv", index=False)
    pd.DataFrame(X_val).to_csv(f"{folder}/X_val.csv", index=False)
    pd.DataFrame(X_test).to_csv(f"{folder}/X_test.csv", index=False)
    pd.Series(y_train).to_csv(f"{folder}/y_train.csv", index=False)
    pd.Series(y_val).to_csv(f"{folder}/y_val.csv", index=False)
    pd.Series(y_test).to_csv(f"{folder}/y_test.csv", index=False)
    
    print(f"   âœ… DonnÃ©es sauvegardÃ©es avec succÃ¨s dans {folder}")
    print(f"     â€¢ 13 fichiers gÃ©nÃ©rÃ©s (6 .pkl, 6 .csv, 1 preprocessor.pkl)")

# Sauvegarde des donnÃ©es pour la suite du projet
save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test, preprocessor)

print("\n" + "="*60)
print("âœ… PRÃ‰PARATION DES DONNÃ‰ES TERMINÃ‰E !")
print("ğŸ¯ Vous pouvez maintenant passer Ã  l'Ã©tape de modÃ©lisation ML")
print("="*60)
