# ===================================================================
# ğŸ”„ SÃ‰PARATION DES DONNÃ‰ES PMR
# ===================================================================

"""
ğŸ¯ Ã‰TAPE 2 : SPLIT TRAIN/VALIDATION/TEST

ğŸ“‹ OBJECTIF :
- Charger le dataset brut unique
- SÃ©parer en train/validation/test (70/15/15%)
- Split stratifiÃ© pour Ã©quilibrer les classes
- Sauvegarde sÃ©parÃ©e pour chaque ensemble
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Configuration
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

def load_raw_dataset():
    """
    ğŸ“‚ CHARGEMENT DU DATASET BRUT
    """
    
    raw_path = Path("../data/raw/pmr_dataset_raw.pkl")
    
    if not raw_path.exists():
        print("âŒ Dataset brut non trouvÃ© !")
        print("ğŸ”§ ExÃ©cutez d'abord : python data_generation.py")
        return None
    
    print(f"ğŸ“‚ Chargement du dataset brut...")
    df = pd.read_pickle(raw_path)
    
    print(f"âœ… Dataset chargÃ© : {df.shape}")
    print(f"ğŸ“Š Colonnes : {list(df.columns)}")
    print(f"ğŸ¯ Target : {df['accessibilite_pmr'].value_counts().sort_index().to_dict()}")
    
    return df

def split_dataset(df, test_size=0.15, val_size=0.15):
    """
    âœ‚ï¸ SÃ‰PARATION STRATIFIÃ‰E DES DONNÃ‰ES
    
    ğŸ“‹ STRATÃ‰GIE :
    - 70% Train (entraÃ®nement des modÃ¨les)
    - 15% Validation (optimisation hyperparamÃ¨tres)
    - 15% Test (Ã©valuation finale)
    - Split stratifiÃ© pour maintenir la distribution des classes
    """
    
    print(f"\nâœ‚ï¸ SÃ©paration des donnÃ©es...")
    print(f"ğŸ“Š RÃ©partition cible : Train 70%, Val 15%, Test 15%")
    
    # SÃ©paration features/target
    X = df.drop('accessibilite_pmr', axis=1)
    y = df['accessibilite_pmr']
    
    print(f"ğŸ“‹ Features : {X.shape}")
    print(f"ğŸ¯ Target : {y.shape}")
    
    # Premier split : Train+Val vs Test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, 
        test_size=test_size,
        stratify=y,
        random_state=RANDOM_STATE
    )
    
    # DeuxiÃ¨me split : Train vs Val
    # val_size ajustÃ© pour obtenir 15% du total
    val_size_adjusted = val_size / (1 - test_size)
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp,
        test_size=val_size_adjusted,
        stratify=y_temp,
        random_state=RANDOM_STATE
    )
    
    # VÃ©rification des dimensions
    total_samples = len(df)
    print(f"\nğŸ“Š RÃ‰SULTATS DU SPLIT :")
    print(f"   ğŸ”¹ Train : {X_train.shape[0]:4d} Ã©chantillons ({X_train.shape[0]/total_samples*100:5.1f}%)")
    print(f"   ğŸ”¹ Val   : {X_val.shape[0]:4d} Ã©chantillons ({X_val.shape[0]/total_samples*100:5.1f}%)")
    print(f"   ğŸ”¹ Test  : {X_test.shape[0]:4d} Ã©chantillons ({X_test.shape[0]/total_samples*100:5.1f}%)")
    
    # VÃ©rification de la stratification
    print(f"\nğŸ¯ VÃ‰RIFICATION STRATIFICATION :")
    for dataset_name, y_set in [("Train", y_train), ("Val", y_val), ("Test", y_test)]:
        dist = y_set.value_counts(normalize=True).sort_index()
        print(f"   {dataset_name:5s} : ", end="")
        for classe, prop in dist.items():
            print(f"Cl{classe}={prop:.1%} ", end="")
        print()
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def save_split_datasets(X_train, X_val, X_test, y_train, y_val, y_test):
    """
    ğŸ’¾ SAUVEGARDE DES DATASETS SÃ‰PARÃ‰S
    """
    
    # CrÃ©ation du dossier split
    split_dir = Path("../data/split")
    split_dir.mkdir(exist_ok=True)
    
    print(f"\nğŸ’¾ Sauvegarde des datasets sÃ©parÃ©s...")
    
    datasets = {
        'train': (X_train, y_train),
        'val': (X_val, y_val),
        'test': (X_test, y_test)
    }
    
    saved_files = []
    
    for split_name, (X, y) in datasets.items():
        # Sauvegarde features
        X_path_csv = split_dir / f"X_{split_name}.csv"
        X_path_pkl = split_dir / f"X_{split_name}.pkl"
        X.to_csv(X_path_csv, index=False)
        X.to_pickle(X_path_pkl)
        
        # Sauvegarde target
        y_path_csv = split_dir / f"y_{split_name}.csv"
        y_path_pkl = split_dir / f"y_{split_name}.pkl"
        y.to_csv(y_path_csv, index=False)
        y.to_pickle(y_path_pkl)
        
        print(f"   âœ… {split_name.upper():5s} : X{X.shape}, y{y.shape}")
        saved_files.extend([X_path_csv, X_path_pkl, y_path_csv, y_path_pkl])
    
    # MÃ©tadonnÃ©es du split
    split_metadata = {
        'split_strategy': 'stratified',
        'train_size': len(X_train),
        'val_size': len(X_val),
        'test_size': len(X_test),
        'train_ratio': len(X_train) / (len(X_train) + len(X_val) + len(X_test)),
        'val_ratio': len(X_val) / (len(X_train) + len(X_val) + len(X_test)),
        'test_ratio': len(X_test) / (len(X_train) + len(X_val) + len(X_test)),
        'random_state': RANDOM_STATE,
        'split_date': pd.Timestamp.now().isoformat()
    }
    
    metadata_path = split_dir / "split_metadata.json"
    import json
    with open(metadata_path, 'w') as f:
        json.dump(split_metadata, f, indent=2)
    
    print(f"   âœ… MÃ©tadonnÃ©es : {metadata_path}")
    
    return saved_files

def verify_split_integrity(X_train, X_val, X_test, y_train, y_val, y_test):
    """
    ğŸ” VÃ‰RIFICATION DE L'INTÃ‰GRITÃ‰ DU SPLIT
    """
    
    print(f"\nğŸ” VÃ‰RIFICATION DE L'INTÃ‰GRITÃ‰...")
    
    # VÃ©rification des dimensions
    assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], "Nombre de features diffÃ©rent !"
    assert len(X_train) == len(y_train), "Tailles X_train et y_train diffÃ©rentes !"
    assert len(X_val) == len(y_val), "Tailles X_val et y_val diffÃ©rentes !"
    assert len(X_test) == len(y_test), "Tailles X_test et y_test diffÃ©rentes !"
    
    # VÃ©rification des valeurs manquantes
    total_missing = (
        X_train.isnull().sum().sum() + 
        X_val.isnull().sum().sum() + 
        X_test.isnull().sum().sum()
    )
    
    print(f"   âœ… Dimensions cohÃ©rentes")
    print(f"   âœ… Correspondance X/y")
    print(f"   âš ï¸  Valeurs manquantes : {total_missing}")
    print(f"   âœ… Split prÃªt pour preprocessing !")

def main():
    """
    ğŸš€ FONCTION PRINCIPALE
    """
    
    print("ğŸš€ SÃ‰PARATION DU DATASET PMR")
    print("=" * 50)
    
    # 1. Chargement du dataset brut
    df = load_raw_dataset()
    if df is None:
        return
    
    # 2. SÃ©paration stratifiÃ©e
    X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df)
    
    # 3. VÃ©rification de l'intÃ©gritÃ©
    verify_split_integrity(X_train, X_val, X_test, y_train, y_val, y_test)
    
    # 4. Sauvegarde
    saved_files = save_split_datasets(X_train, X_val, X_test, y_train, y_val, y_test)
    
    print(f"\nğŸ‰ SÃ‰PARATION TERMINÃ‰E AVEC SUCCÃˆS !")
    print(f"ğŸ“ {len(saved_files)} fichiers crÃ©Ã©s dans /data/split/")
    print(f"ğŸ”„ PrÃªt pour l'Ã©tape de preprocessing !")

if __name__ == "__main__":
    main()
